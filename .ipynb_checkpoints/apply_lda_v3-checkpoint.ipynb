{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import string\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "# text\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "# sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2008_44_pid=76232.txt'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getcwd()+'\\\\data'\n",
    "fileList = os.listdir(path)\n",
    "\n",
    "fileList[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank you, Iowa',\n",
       " ' You know, they said this day would never come',\n",
       " ' They said our sights were set too high',\n",
       " ' They said this country was too divided; too disillusioned to ever come together around a common purpose',\n",
       " \" But on this January night - at this defining moment in history - you have done what the cynics said we couldn't do\",\n",
       " ' You have done what the state of New Hampshire can do in five days',\n",
       " ' You have done what America can do in this New Year, 2008',\n",
       " ' In lines that stretched around schools and churches; in small towns and big cities; you came together as Democrats, Republicans and Independents to stand up and say that we are one nation; we are one people; and our time for change has come',\n",
       " \" You said the time has come to move beyond the bitterness and pettiness and anger that's consumed Washington; to end the political strategy that's been all about division and instead make it about addition - to build a coalition for change that stretches through Red States and Blue States\",\n",
       " \" Because that's how we'll win in November, and that's how we'll finally meet the challenges that we face as a nation\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathToFile = path + '\\\\' + fileList[3]\n",
    "with open(pathToFile, 'r') as f:\n",
    "    txt = f.readlines()\n",
    "    \n",
    "txtList = txt[0].split('.')\n",
    "txtList[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "stop += ['.', ',', '(', ')', \"'\", '\"']\n",
    "\n",
    "def stringClean(str):\n",
    "    str = ''.join([i for i in str if i not in string.punctuation])\n",
    "    words = TextBlob(str).words\n",
    "    words = ' '.join([w for w in words if w not in stop if len(w)> 1])\n",
    "    \n",
    "    return words\n",
    "\n",
    "updatedTextList = []\n",
    "for txt in txtList:\n",
    "    updatedTextList.append(stringClean(txt))\n",
    "    \n",
    "updatedTextList[:5]\n",
    "len(updatedTextList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='\\\\b[a-z][a-z]+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a CountVectorizer for parsing/counting words\n",
    "# make sure we are breaking out by word and not character.\n",
    "doc_term_vector = CountVectorizer(analyzer='word',\n",
    "                                  ngram_range=(1,2),\n",
    "                                  stop_words='english',\n",
    "                                  token_pattern = '\\\\b[a-z][a-z]+\\\\b'\n",
    "                                 )\n",
    "\n",
    "doc_term_vector.fit(txtList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able', 'able breathe', 'able look', 'addition', 'addition build']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_vector.get_feature_names()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(823, 47)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the term-document matrix\n",
    "# Transpose it so the terms are the rows\n",
    "term_docs = doc_term_vector.transform(updatedTextList).transpose()\n",
    "term_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "# here corpus will be fed into lda in iteration\n",
    "corpus = matutils.Sparse2Corpus(term_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2word = dict((v,k) for k, v in doc_term_vector.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "# we can also run lsa on this:\n",
    "# lsa = models.LsiModel(corpus=corpus, num_topics)\n",
    "lda = models.LdaModel(corpus=corpus, num_topics=10, id2word=id2word, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*thank + 0.009*choosing + 0.009*division + 0.009*especially like + 0.009*sending'),\n",
       " (1,\n",
       "  '0.014*hope + 0.010*president + 0.010*know + 0.007*new + 0.007*hampshire'),\n",
       " (2,\n",
       "  '0.015*young + 0.015*night + 0.015*woman + 0.015*young woman + 0.008*moment'),\n",
       " (3, '0.014*hope + 0.014*little + 0.014*america + 0.014*world + 0.007*day'),\n",
       " (4,\n",
       "  '0.019*states + 0.015*iowa + 0.015*thank + 0.010*care + 0.010*health care'),\n",
       " (5, '0.019*led + 0.013*nation + 0.013*free + 0.013*young + 0.007*president'),\n",
       " (6, '0.013*know + 0.013*people + 0.013*stand + 0.007*hard + 0.007*know hard'),\n",
       " (7,\n",
       "  '0.012*change + 0.012*face + 0.012*finally + 0.012*threats + 0.012*common threats'),\n",
       " (8, '0.014*said + 0.014*moment + 0.009*come + 0.009*think + 0.009*days'),\n",
       " (9,\n",
       "  '0.016*politics + 0.008*hope + 0.008*moment + 0.008*finally + 0.008*fear')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(num_words=5, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(408, 0.014951085364410407),\n",
       " (483, 0.014942603465743614),\n",
       " (337, 0.010129232864630223),\n",
       " (102, 0.010128715329547759),\n",
       " (817, 0.010128259597981019),\n",
       " (618, 0.0053070292962610725),\n",
       " (332, 0.0053065738275374641),\n",
       " (193, 0.0053065689793565726),\n",
       " (201, 0.0053065443770853333),\n",
       " (192, 0.0053065433258200458)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we are looking into top words that pertain to topic 0\n",
    "lda.get_topic_terms(2,topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_corpus = lda[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_docs = [doc for doc in lda_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.025000000117549978),\n",
       " (1, 0.025000721463372268),\n",
       " (2, 0.025000000083206794),\n",
       " (3, 0.025003397250346206),\n",
       " (4, 0.77499400909080118),\n",
       " (5, 0.025000000126935967),\n",
       " (6, 0.025000000125384444),\n",
       " (7, 0.025001202016736005),\n",
       " (8, 0.025000669559327497),\n",
       " (9, 0.025000000166339856)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.matutils.Sparse2Corpus at 0x7973770>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'able',\n",
       " 1: 'able breathe',\n",
       " 2: 'able look',\n",
       " 3: 'addition',\n",
       " 4: 'addition build',\n",
       " 5: 'afford',\n",
       " 6: 'afford doctor',\n",
       " 7: 'afford health',\n",
       " 8: 'affordable',\n",
       " 9: 'affordable available',\n",
       " 10: 'ages',\n",
       " 11: 'ages common',\n",
       " 12: 'ahead',\n",
       " 13: 'ahead roadblocks',\n",
       " 14: 'america',\n",
       " 15: 'america did',\n",
       " 16: 'america differently',\n",
       " 17: 'america moment',\n",
       " 18: 'america new',\n",
       " 19: 'america remembered',\n",
       " 20: 'america sees',\n",
       " 21: 'america world',\n",
       " 22: 'american',\n",
       " 23: 'american ideas',\n",
       " 24: 'american way',\n",
       " 25: 'americans',\n",
       " 26: 'americans deserve',\n",
       " 27: 'americans participated',\n",
       " 28: 'anger',\n",
       " 29: 'anger consumed',\n",
       " 30: 'available',\n",
       " 31: 'available single',\n",
       " 32: 'awaits',\n",
       " 33: 'awaits courage',\n",
       " 34: 'band',\n",
       " 35: 'band colonists',\n",
       " 36: 'barriers',\n",
       " 37: 'barriers divided',\n",
       " 38: 'beat',\n",
       " 39: 'beat politics',\n",
       " 40: 'beat washington',\n",
       " 41: 'bed',\n",
       " 42: 'bed night',\n",
       " 43: 'bedrock',\n",
       " 44: 'bedrock nation',\n",
       " 45: 'began',\n",
       " 46: 'began streets',\n",
       " 47: 'belief',\n",
       " 48: 'belief destiny',\n",
       " 49: 'believe',\n",
       " 50: 'believe families',\n",
       " 51: 'believed',\n",
       " 52: 'believed deeply',\n",
       " 53: 'believes',\n",
       " 54: 'believes country',\n",
       " 55: 'better',\n",
       " 56: 'better awaits',\n",
       " 57: 'big',\n",
       " 58: 'big cities',\n",
       " 59: 'bit',\n",
       " 60: 'bit better',\n",
       " 61: 'bitterness',\n",
       " 62: 'bitterness pettiness',\n",
       " 63: 'blind',\n",
       " 64: 'blind optimism',\n",
       " 65: 'block',\n",
       " 66: 'block block',\n",
       " 67: 'block calloused',\n",
       " 68: 'blue',\n",
       " 69: 'blue states',\n",
       " 70: 'brave',\n",
       " 71: 'brave hoses',\n",
       " 72: 'breaks',\n",
       " 73: 'breaks companies',\n",
       " 74: 'breathe',\n",
       " 75: 'breathe nephew',\n",
       " 76: 'brick',\n",
       " 77: 'brick block',\n",
       " 78: 'brick brick',\n",
       " 79: 'bringing',\n",
       " 80: 'bringing democrats',\n",
       " 81: 'brings',\n",
       " 82: 'brings troops',\n",
       " 83: 'build',\n",
       " 84: 'build coalition',\n",
       " 85: 'calloused',\n",
       " 86: 'calloused hand',\n",
       " 87: 'came',\n",
       " 88: 'came democrats',\n",
       " 89: 'campaign',\n",
       " 90: 'campaign campaigns',\n",
       " 91: 'campaign trail',\n",
       " 92: 'campaigns',\n",
       " 93: 'campaigns iowa',\n",
       " 94: 'captains',\n",
       " 95: 'captains volunteers',\n",
       " 96: 'care',\n",
       " 97: 'care affordable',\n",
       " 98: 'care illinois',\n",
       " 99: 'care sister',\n",
       " 100: 'carry',\n",
       " 101: 'carry new',\n",
       " 102: 'cause',\n",
       " 103: 'cause finally',\n",
       " 104: 'cedar',\n",
       " 105: 'cedar rapids',\n",
       " 106: 'century',\n",
       " 107: 'century common',\n",
       " 108: 'challenge',\n",
       " 109: 'challenge unite',\n",
       " 110: 'challenges',\n",
       " 111: 'challenges face',\n",
       " 112: 'chance',\n",
       " 113: 'chance iowa',\n",
       " 114: 'chance live',\n",
       " 115: 'change',\n",
       " 116: 'change come',\n",
       " 117: 'change coming',\n",
       " 118: 'change country',\n",
       " 119: 'change poverty',\n",
       " 120: 'change stretches',\n",
       " 121: 'changes',\n",
       " 122: 'changes believe',\n",
       " 123: 'chicago',\n",
       " 124: 'chicago doing',\n",
       " 125: 'children',\n",
       " 126: 'children inherit',\n",
       " 127: 'children malia',\n",
       " 128: 'choices',\n",
       " 129: 'choices challenges',\n",
       " 130: 'choosing',\n",
       " 131: 'choosing hope',\n",
       " 132: 'choosing unity',\n",
       " 133: 'churches',\n",
       " 134: 'churches small',\n",
       " 135: 'cities',\n",
       " 136: 'cities came',\n",
       " 137: 'class',\n",
       " 138: 'class tax',\n",
       " 139: 'cleaner',\n",
       " 140: 'cleaner safer',\n",
       " 141: 'climate',\n",
       " 142: 'climate change',\n",
       " 143: 'closer',\n",
       " 144: 'closer campaign',\n",
       " 145: 'closer vision',\n",
       " 146: 'coalition',\n",
       " 147: 'coalition change',\n",
       " 148: 'collection',\n",
       " 149: 'collection red',\n",
       " 150: 'college',\n",
       " 151: 'college afford',\n",
       " 152: 'colonists',\n",
       " 153: 'colonists rise',\n",
       " 154: 'come',\n",
       " 155: 'come bitterness',\n",
       " 156: 'come common',\n",
       " 157: 'come president',\n",
       " 158: 'come tell',\n",
       " 159: 'comes',\n",
       " 160: 'comes little',\n",
       " 161: 'coming',\n",
       " 162: 'coming america',\n",
       " 163: 'common',\n",
       " 164: 'common cause',\n",
       " 165: 'common purpose',\n",
       " 166: 'common threats',\n",
       " 167: 'companies',\n",
       " 168: 'companies ship',\n",
       " 169: 'consumed',\n",
       " 170: 'consumed washington',\n",
       " 171: 'content',\n",
       " 172: 'content settle',\n",
       " 173: 'continent',\n",
       " 174: 'continent heal',\n",
       " 175: 'contrary',\n",
       " 176: 'contrary better',\n",
       " 177: 'couldn',\n",
       " 178: 'counters',\n",
       " 179: 'counters brave',\n",
       " 180: 'country',\n",
       " 181: 'country brick',\n",
       " 182: 'country chance',\n",
       " 183: 'country change',\n",
       " 184: 'country divided',\n",
       " 185: 'courage',\n",
       " 186: 'courage reach',\n",
       " 187: 'courage remake',\n",
       " 188: 'cut',\n",
       " 189: 'cut pockets',\n",
       " 190: 'cynicism',\n",
       " 191: 'cynicism politics',\n",
       " 192: 'cynics',\n",
       " 193: 'cynics said',\n",
       " 194: 'day',\n",
       " 195: 'day college',\n",
       " 196: 'day come',\n",
       " 197: 'days',\n",
       " 198: 'days disappointment',\n",
       " 199: 'deeply',\n",
       " 200: 'deeply american',\n",
       " 201: 'defining',\n",
       " 202: 'defining moment',\n",
       " 203: 'democrats',\n",
       " 204: 'democrats republicans',\n",
       " 205: 'derided',\n",
       " 206: 'derided talking',\n",
       " 207: 'deserve',\n",
       " 208: 'despite',\n",
       " 209: 'despite evidence',\n",
       " 210: 'destiny',\n",
       " 211: 'destiny written',\n",
       " 212: 'did',\n",
       " 213: 'did believed',\n",
       " 214: 'did did',\n",
       " 215: 'did iowa',\n",
       " 216: 'did tonight',\n",
       " 217: 'didn',\n",
       " 218: 'differently',\n",
       " 219: 'differently america',\n",
       " 220: 'disagree',\n",
       " 221: 'disagree won',\n",
       " 222: 'disappointment',\n",
       " 223: 'disappointment just',\n",
       " 224: 'disease',\n",
       " 225: 'disillusioned',\n",
       " 226: 'disillusioned come',\n",
       " 227: 'divided',\n",
       " 228: 'divided disillusioned',\n",
       " 229: 'divided long',\n",
       " 230: 'divided united',\n",
       " 231: 'division',\n",
       " 232: 'division instead',\n",
       " 233: 'division sending',\n",
       " 234: 'doctor',\n",
       " 235: 'doctor children',\n",
       " 236: 'doing',\n",
       " 237: 'doing campaign',\n",
       " 238: 'don',\n",
       " 239: 'don government',\n",
       " 240: 'doubt',\n",
       " 241: 'doubt cynicism',\n",
       " 242: 'dreams',\n",
       " 243: 'election',\n",
       " 244: 'election ready',\n",
       " 245: 'empire',\n",
       " 246: 'empire led',\n",
       " 247: 'end',\n",
       " 248: 'end political',\n",
       " 249: 'ends',\n",
       " 250: 'ends tax',\n",
       " 251: 'ends war',\n",
       " 252: 'enormity',\n",
       " 253: 'enormity task',\n",
       " 254: 'entrepreneurs',\n",
       " 255: 'entrepreneurs free',\n",
       " 256: 'especially',\n",
       " 257: 'especially like',\n",
       " 258: 'evidence',\n",
       " 259: 'evidence contrary',\n",
       " 260: 'expanded',\n",
       " 261: 'expanded health',\n",
       " 262: 'extraordinary',\n",
       " 263: 'extraordinary things',\n",
       " 264: 'eyes',\n",
       " 265: 'eyes young',\n",
       " 266: 'face',\n",
       " 267: 'face impossible',\n",
       " 268: 'face listen',\n",
       " 269: 'face nation',\n",
       " 270: 'families',\n",
       " 271: 'families afford',\n",
       " 272: 'family',\n",
       " 273: 'family closer',\n",
       " 274: 'farmers',\n",
       " 275: 'farmers scientists',\n",
       " 276: 'father',\n",
       " 277: 'father kenya',\n",
       " 278: 'fear',\n",
       " 279: 'fear doubt',\n",
       " 280: 'fight',\n",
       " 281: 'fighting',\n",
       " 282: 'fighting make',\n",
       " 283: 'finally',\n",
       " 284: 'finally beat',\n",
       " 285: 'finally brings',\n",
       " 286: 'finally gave',\n",
       " 287: 'finally makes',\n",
       " 288: 'finally meet',\n",
       " 289: 'forget',\n",
       " 290: 'forget journey',\n",
       " 291: 'free',\n",
       " 292: 'free continent',\n",
       " 293: 'free nation',\n",
       " 294: 'freedom',\n",
       " 295: 'freedom cause',\n",
       " 296: 'gave',\n",
       " 297: 'gave americans',\n",
       " 298: 'generations',\n",
       " 299: 'generations free',\n",
       " 300: 'genocide',\n",
       " 301: 'genocide disease',\n",
       " 302: 'goes',\n",
       " 303: 'goes bed',\n",
       " 304: 'government',\n",
       " 305: 'greatest',\n",
       " 306: 'greatest generations',\n",
       " 307: 'hampshire',\n",
       " 308: 'hampshire chance',\n",
       " 309: 'hampshire days',\n",
       " 310: 'hampshire message',\n",
       " 311: 'hampshire woman',\n",
       " 312: 'hand',\n",
       " 313: 'hand calloused',\n",
       " 314: 'hand ordinary',\n",
       " 315: 'happen',\n",
       " 316: 'happen united',\n",
       " 317: 'hard',\n",
       " 318: 'harnesses',\n",
       " 319: 'harnesses ingenuity',\n",
       " 320: 'hasn',\n",
       " 321: 'hasn able',\n",
       " 322: 'heal',\n",
       " 323: 'heal nation',\n",
       " 324: 'health',\n",
       " 325: 'health care',\n",
       " 326: 'hear',\n",
       " 327: 'hear need',\n",
       " 328: 'heard',\n",
       " 329: 'heard voice',\n",
       " 330: 'high',\n",
       " 331: 'history',\n",
       " 332: 'history cynics',\n",
       " 333: 'home',\n",
       " 334: 'home restores',\n",
       " 335: 'honest',\n",
       " 336: 'honest choices',\n",
       " 337: 'hope',\n",
       " 338: 'hope bedrock',\n",
       " 339: 'hope blind',\n",
       " 340: 'hope fear',\n",
       " 341: 'hope heard',\n",
       " 342: 'hope hope',\n",
       " 343: 'hope led',\n",
       " 344: 'hope saw',\n",
       " 345: 'hope thing',\n",
       " 346: 'hoses',\n",
       " 347: 'hoses march',\n",
       " 348: 'ideas',\n",
       " 349: 'ideas face',\n",
       " 350: 'ignoring',\n",
       " 351: 'ignoring enormity',\n",
       " 352: 'ill',\n",
       " 353: 'ill young',\n",
       " 354: 'illinois',\n",
       " 355: 'illinois bringing',\n",
       " 356: 'impossible',\n",
       " 357: 'impossible odds',\n",
       " 358: 'improbable',\n",
       " 359: 'improbable beat',\n",
       " 360: 'independents',\n",
       " 361: 'independents stand',\n",
       " 362: 'inevitable',\n",
       " 363: 'influence',\n",
       " 364: 'influence speak',\n",
       " 365: 'ingenuity',\n",
       " 366: 'ingenuity farmers',\n",
       " 367: 'inherit',\n",
       " 368: 'inherit planet',\n",
       " 369: 'inside',\n",
       " 370: 'inside insists',\n",
       " 371: 'insists',\n",
       " 372: 'insists despite',\n",
       " 373: 'instead',\n",
       " 374: 'instead lifting',\n",
       " 375: 'instead make',\n",
       " 376: 'iowa',\n",
       " 377: 'iowa did',\n",
       " 378: 'iowa message',\n",
       " 379: 'iowa organizing',\n",
       " 380: 'iraq',\n",
       " 381: 'iraq finally',\n",
       " 382: 'iraq goes',\n",
       " 383: 'january',\n",
       " 384: 'january night',\n",
       " 385: 'job',\n",
       " 386: 'jobs',\n",
       " 387: 'jobs overseas',\n",
       " 388: 'journey',\n",
       " 389: 'journey began',\n",
       " 390: 'just',\n",
       " 391: 'just little',\n",
       " 392: 'just nights',\n",
       " 393: 'just tell',\n",
       " 394: 'kansas',\n",
       " 395: 'kansas story',\n",
       " 396: 'kenya',\n",
       " 397: 'kenya mother',\n",
       " 398: 'knew',\n",
       " 399: 'knew hope',\n",
       " 400: 'know',\n",
       " 401: 'know didn',\n",
       " 402: 'know hard',\n",
       " 403: 'know know',\n",
       " 404: 'know said',\n",
       " 405: 'know standing',\n",
       " 406: 'learn',\n",
       " 407: 'learn disagree',\n",
       " 408: 'led',\n",
       " 409: 'led band',\n",
       " 410: 'led greatest',\n",
       " 411: 'led today',\n",
       " 412: 'led young',\n",
       " 413: 'left',\n",
       " 414: 'left iraq',\n",
       " 415: 'life',\n",
       " 416: 'life rock',\n",
       " 417: 'lifting',\n",
       " 418: 'lifting country',\n",
       " 419: 'like',\n",
       " 420: 'like night',\n",
       " 421: 'like thank',\n",
       " 422: 'lines',\n",
       " 423: 'lines stretched',\n",
       " 424: 'listen',\n",
       " 425: 'listen learn',\n",
       " 426: 'little',\n",
       " 427: 'little bit',\n",
       " 428: 'little cleaner',\n",
       " 429: 'little pay',\n",
       " 430: 'little sleep',\n",
       " 431: 'live',\n",
       " 432: 'live dreams',\n",
       " 433: 'lives',\n",
       " 434: 'lives just',\n",
       " 435: 'll',\n",
       " 436: 'll able',\n",
       " 437: 'll finally',\n",
       " 438: 'll forget',\n",
       " 439: 'll look',\n",
       " 440: 'll president',\n",
       " 441: 'll say',\n",
       " 442: 'll win',\n",
       " 443: 'lobbyists',\n",
       " 444: 'lobbyists think',\n",
       " 445: 'long',\n",
       " 446: 'long rallied',\n",
       " 447: 'look',\n",
       " 448: 'look ll',\n",
       " 449: 'look pride',\n",
       " 450: 'lot',\n",
       " 451: 'lot sacrifice',\n",
       " 452: 'louder',\n",
       " 453: 'louder voices',\n",
       " 454: 'love',\n",
       " 455: 'love country',\n",
       " 456: 'love life',\n",
       " 457: 'lunch',\n",
       " 458: 'lunch counters',\n",
       " 459: 'make',\n",
       " 460: 'make addition',\n",
       " 461: 'make people',\n",
       " 462: 'makes',\n",
       " 463: 'makes health',\n",
       " 464: 'makes sense',\n",
       " 465: 'malia',\n",
       " 466: 'malia sasha',\n",
       " 467: 'march',\n",
       " 468: 'march selma',\n",
       " 469: 'means',\n",
       " 470: 'means hope',\n",
       " 471: 'meet',\n",
       " 472: 'meet challenges',\n",
       " 473: 'men',\n",
       " 474: 'men sit',\n",
       " 475: 'men women',\n",
       " 476: 'message',\n",
       " 477: 'message carry',\n",
       " 478: 'message change',\n",
       " 479: 'michelle',\n",
       " 480: 'michelle obama',\n",
       " 481: 'middle',\n",
       " 482: 'middle class',\n",
       " 483: 'moment',\n",
       " 484: 'moment began',\n",
       " 485: 'moment election',\n",
       " 486: 'moment finally',\n",
       " 487: 'moment history',\n",
       " 488: 'moment improbable',\n",
       " 489: 'moment place',\n",
       " 490: 'moment tore',\n",
       " 491: 'money',\n",
       " 492: 'money influence',\n",
       " 493: 'montgomery',\n",
       " 494: 'montgomery freedom',\n",
       " 495: 'months',\n",
       " 496: 'months ve',\n",
       " 497: 'moral',\n",
       " 498: 'moral standing',\n",
       " 499: 'mother',\n",
       " 500: 'mother kansas',\n",
       " 501: 'nation',\n",
       " 502: 'nation belief',\n",
       " 503: 'nation divided',\n",
       " 504: 'nation led',\n",
       " 505: 'nation people',\n",
       " 506: 'nation tyranny',\n",
       " 507: 'need',\n",
       " 508: 'need know',\n",
       " 509: 'nephew',\n",
       " 510: 'nephew left',\n",
       " 511: 'new',\n",
       " 512: 'new hampshire',\n",
       " 513: 'new year',\n",
       " 514: 'night',\n",
       " 515: 'night defining',\n",
       " 516: 'night night',\n",
       " 517: 'night praying',\n",
       " 518: 'night shift',\n",
       " 519: 'night years',\n",
       " 520: 'nights',\n",
       " 521: 'nights like',\n",
       " 522: 'november',\n",
       " 523: 'november ll',\n",
       " 524: 'nuclear',\n",
       " 525: 'nuclear weapons',\n",
       " 526: 'obama',\n",
       " 527: 'obama family',\n",
       " 528: 'odds',\n",
       " 529: 'odds people',\n",
       " 530: 'oil',\n",
       " 531: 'optimism',\n",
       " 532: 'ordinary',\n",
       " 533: 'ordinary people',\n",
       " 534: 'organizers',\n",
       " 535: 'organizers precinct',\n",
       " 536: 'organizing',\n",
       " 537: 'organizing working',\n",
       " 538: 'overseas',\n",
       " 539: 'overseas middle',\n",
       " 540: 'participated',\n",
       " 541: 'participated politics',\n",
       " 542: 'parties',\n",
       " 543: 'parties ages',\n",
       " 544: 'path',\n",
       " 545: 'pay',\n",
       " 546: 'pay lot',\n",
       " 547: 'people',\n",
       " 548: 'people extraordinary',\n",
       " 549: 'people lives',\n",
       " 550: 'people love',\n",
       " 551: 'people parties',\n",
       " 552: 'people time',\n",
       " 553: 'pettiness',\n",
       " 554: 'pettiness anger',\n",
       " 555: 'place',\n",
       " 556: 'place america',\n",
       " 557: 'planet',\n",
       " 558: 'planet little',\n",
       " 559: 'pockets',\n",
       " 560: 'pockets working',\n",
       " 561: 'political',\n",
       " 562: 'political strategy',\n",
       " 563: 'politics',\n",
       " 564: 'politics fear',\n",
       " 565: 'politics reason',\n",
       " 566: 'politics tear',\n",
       " 567: 'possible',\n",
       " 568: 'poverty',\n",
       " 569: 'poverty genocide',\n",
       " 570: 'powerful',\n",
       " 571: 'powerful message',\n",
       " 572: 'praying',\n",
       " 573: 'praying safe',\n",
       " 574: 'precinct',\n",
       " 575: 'precinct captains',\n",
       " 576: 'president',\n",
       " 577: 'president america',\n",
       " 578: 'president ends',\n",
       " 579: 'president finally',\n",
       " 580: 'president harnesses',\n",
       " 581: 'president honest',\n",
       " 582: 'pride',\n",
       " 583: 'pride say',\n",
       " 584: 'purpose',\n",
       " 585: 'rallied',\n",
       " 586: 'rallied people',\n",
       " 587: 'rapids',\n",
       " 588: 'rapids works',\n",
       " 589: 'reach',\n",
       " 590: 'reach work',\n",
       " 591: 'ready',\n",
       " 592: 'ready believe',\n",
       " 593: 'reason',\n",
       " 594: 'reason stand',\n",
       " 595: 'red',\n",
       " 596: 'red states',\n",
       " 597: 'remake',\n",
       " 598: 'remake world',\n",
       " 599: 'remembered',\n",
       " 600: 'remembered means',\n",
       " 601: 'republicans',\n",
       " 602: 'republicans independents',\n",
       " 603: 'republicans job',\n",
       " 604: 'restores',\n",
       " 605: 'restores moral',\n",
       " 606: 'return',\n",
       " 607: 'rise',\n",
       " 608: 'rise empire',\n",
       " 609: 'roadblocks',\n",
       " 610: 'roadblocks stand',\n",
       " 611: 'rock',\n",
       " 612: 'rock obama',\n",
       " 613: 'sacrifice',\n",
       " 614: 'safe',\n",
       " 615: 'safe return',\n",
       " 616: 'safer',\n",
       " 617: 'safer world',\n",
       " 618: 'said',\n",
       " 619: 'said couldn',\n",
       " 620: 'said country',\n",
       " 621: 'said day',\n",
       " 622: 'said inevitable',\n",
       " 623: 'said sights',\n",
       " 624: 'said time',\n",
       " 625: 'sasha',\n",
       " 626: 'sasha children',\n",
       " 627: 'saw',\n",
       " 628: 'saw eyes',\n",
       " 629: 'say',\n",
       " 630: 'say moment',\n",
       " 631: 'say nation',\n",
       " 632: 'scare',\n",
       " 633: 'scare votes',\n",
       " 634: 'schools',\n",
       " 635: 'schools churches',\n",
       " 636: 'scientists',\n",
       " 637: 'scientists entrepreneurs',\n",
       " 638: 'sees',\n",
       " 639: 'sees america',\n",
       " 640: 'sees nation',\n",
       " 641: 'selma',\n",
       " 642: 'selma montgomery',\n",
       " 643: 'sending',\n",
       " 644: 'sending powerful',\n",
       " 645: 'sense',\n",
       " 646: 'sense thank',\n",
       " 647: 'set',\n",
       " 648: 'set high',\n",
       " 649: 'settle',\n",
       " 650: 'settle world',\n",
       " 651: 'shift',\n",
       " 652: 'shift day',\n",
       " 653: 'ship',\n",
       " 654: 'ship jobs',\n",
       " 655: 'shirking',\n",
       " 656: 'shirking fight',\n",
       " 657: 'sidelines',\n",
       " 658: 'sidelines shirking',\n",
       " 659: 'sights',\n",
       " 660: 'sights set',\n",
       " 661: 'single',\n",
       " 662: 'single american',\n",
       " 663: 'sister',\n",
       " 664: 'sister ill',\n",
       " 665: 'sit',\n",
       " 666: 'sit lunch',\n",
       " 667: 'sitting',\n",
       " 668: 'sitting sidelines',\n",
       " 669: 'sleep',\n",
       " 670: 'sleep little',\n",
       " 671: 'small',\n",
       " 672: 'small towns',\n",
       " 673: 'speak',\n",
       " 674: 'speak louder',\n",
       " 675: 'staff',\n",
       " 676: 'staff possible',\n",
       " 677: 'stand',\n",
       " 678: 'stand path',\n",
       " 679: 'stand say',\n",
       " 680: 'standing',\n",
       " 681: 'standing tonight',\n",
       " 682: 'standing understands',\n",
       " 683: 'started',\n",
       " 684: 'started iowa',\n",
       " 685: 'state',\n",
       " 686: 'state new',\n",
       " 687: 'states',\n",
       " 688: 'states america',\n",
       " 689: 'states blue',\n",
       " 690: 'states united',\n",
       " 691: 'step',\n",
       " 692: 'step closer',\n",
       " 693: 'story',\n",
       " 694: 'story happen',\n",
       " 695: 'strategy',\n",
       " 696: 'strategy division',\n",
       " 697: 'streets',\n",
       " 698: 'streets chicago',\n",
       " 699: 'stretched',\n",
       " 700: 'stretched schools',\n",
       " 701: 'stretches',\n",
       " 702: 'stretches red',\n",
       " 703: 'talking',\n",
       " 704: 'talking hope',\n",
       " 705: 'task',\n",
       " 706: 'task ahead',\n",
       " 707: 'tax',\n",
       " 708: 'tax breaks',\n",
       " 709: 'tax cut',\n",
       " 710: 'tear',\n",
       " 711: 'tear instead',\n",
       " 712: 'teased',\n",
       " 713: 'teased derided',\n",
       " 714: 'tell',\n",
       " 715: 'tell lobbyists',\n",
       " 716: 'tell want',\n",
       " 717: 'terrorism',\n",
       " 718: 'terrorism nuclear',\n",
       " 719: 'thank',\n",
       " 720: 'thank iowa',\n",
       " 721: 'thank love',\n",
       " 722: 'thank organizers',\n",
       " 723: 'thank yous',\n",
       " 724: 'thing',\n",
       " 725: 'thing inside',\n",
       " 726: 'things',\n",
       " 727: 'things collection',\n",
       " 728: 'think',\n",
       " 729: 'think makes',\n",
       " 730: 'think money',\n",
       " 731: 'threats',\n",
       " 732: 'threats century',\n",
       " 733: 'threats terrorism',\n",
       " 734: 'time',\n",
       " 735: 'time change',\n",
       " 736: 'time come',\n",
       " 737: 'today',\n",
       " 738: 'today father',\n",
       " 739: 'told',\n",
       " 740: 'told hasn',\n",
       " 741: 'tonight',\n",
       " 742: 'tonight ll',\n",
       " 743: 'tonight president',\n",
       " 744: 'tonight step',\n",
       " 745: 'tore',\n",
       " 746: 'tore barriers',\n",
       " 747: 'towns',\n",
       " 748: 'towns big',\n",
       " 749: 'trail',\n",
       " 750: 'trail michelle',\n",
       " 751: 'troops',\n",
       " 752: 'troops home',\n",
       " 753: 'tyranny',\n",
       " 754: 'tyranny oil',\n",
       " 755: 'understands',\n",
       " 756: 'understands way',\n",
       " 757: 'unite',\n",
       " 758: 'unite america',\n",
       " 759: 'united',\n",
       " 760: 'united ll',\n",
       " 761: 'united states',\n",
       " 762: 'unity',\n",
       " 763: 'unity division',\n",
       " 764: 've',\n",
       " 765: 've changes',\n",
       " 766: 've teased',\n",
       " 767: 'vision',\n",
       " 768: 'vision america',\n",
       " 769: 'voice',\n",
       " 770: 'voice new',\n",
       " 771: 'voices',\n",
       " 772: 'voices don',\n",
       " 773: 'volunteers',\n",
       " 774: 'volunteers staff',\n",
       " 775: 'votes',\n",
       " 776: 'votes challenge',\n",
       " 777: 'want',\n",
       " 778: 'want hear',\n",
       " 779: 'war',\n",
       " 780: 'war iraq',\n",
       " 781: 'washington',\n",
       " 782: 'washington end',\n",
       " 783: 'washington said',\n",
       " 784: 'way',\n",
       " 785: 'way expanded',\n",
       " 786: 'way scare',\n",
       " 787: 'weapons',\n",
       " 788: 'weapons climate',\n",
       " 789: 'win',\n",
       " 790: 'win november',\n",
       " 791: 'woman',\n",
       " 792: 'woman believes',\n",
       " 793: 'woman cedar',\n",
       " 794: 'woman told',\n",
       " 795: 'women',\n",
       " 796: 'women content',\n",
       " 797: 'women young',\n",
       " 798: 'won',\n",
       " 799: 'won just',\n",
       " 800: 'work',\n",
       " 801: 'work fight',\n",
       " 802: 'working',\n",
       " 803: 'working americans',\n",
       " 804: 'working fighting',\n",
       " 805: 'works',\n",
       " 806: 'works night',\n",
       " 807: 'world',\n",
       " 808: 'world common',\n",
       " 809: 'world courage',\n",
       " 810: 'world sees',\n",
       " 811: 'written',\n",
       " 812: 'written men',\n",
       " 813: 'year',\n",
       " 814: 'years',\n",
       " 815: 'years ll',\n",
       " 816: 'years ve',\n",
       " 817: 'young',\n",
       " 818: 'young men',\n",
       " 819: 'young woman',\n",
       " 820: 'young women',\n",
       " 821: 'yous',\n",
       " 822: 'yous think'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# above we see that word id 376 has the highest\n",
    "# probability of being in topic 0\n",
    "# from below the word is iowa\n",
    "lda.id2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how about we combined many texts together and try this out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()+'\\\\data'\n",
    "fileList = os.listdir(path)\n",
    "\n",
    "# in the part of file 'speech_data_pull_v3.ipynb' I have \n",
    "pathToFile = path + '\\\\' + 'bulkSpeeches.txt'\n",
    "with open(pathToFile, 'r') as f:\n",
    "    txt = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30145"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtList = txt[0].split('.')\n",
    "txtList[:5]\n",
    "len(txtList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wherever go talk need bring real change country',\n",
       " 'And understand need change well folks Michigan',\n",
       " 'Because weve talking recession country months Michigan living long time',\n",
       " 'Michigan highest unemployment rate nation workers communities across state struggling years downturn America feeling today',\n",
       " 'In fairness challenges product larger forces beyond control government']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "stop += ['.', ',', '(', ')', \"'\", '\"']\n",
    "\n",
    "def stringClean(str):\n",
    "    str = ''.join([i for i in str if i not in string.punctuation])\n",
    "    words = TextBlob(str).words\n",
    "    words = ' '.join([w for w in words if w not in stop if len(w)> 1])\n",
    "    \n",
    "    return words\n",
    "\n",
    "updatedTextList = []\n",
    "for txt in txtList:\n",
    "    updatedTextList.append(stringClean(txt))\n",
    "    \n",
    "updatedTextList[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30145"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(updatedTextList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='\\\\b[a-z][a-z]+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a CountVectorizer for parsing/counting words\n",
    "# make sure we are breaking out by word and not character.\n",
    "doc_term_vector = CountVectorizer(analyzer='word',\n",
    "                                  ngram_range=(1,2),\n",
    "                                  stop_words='english',\n",
    "                                  token_pattern = '\\\\b[a-z][a-z]+\\\\b'\n",
    "                                 )\n",
    "\n",
    "doc_term_vector.fit(updatedTextList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aan', 'aan energy', 'aaron', 'aaron joshua', 'aarp']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_vector.get_feature_names()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136374, 30145)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the term-document matrix\n",
    "# Transpose it so the terms are the rows\n",
    "term_docs = doc_term_vector.transform(updatedTextList).transpose()\n",
    "term_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "# here corpus will be fed into lda in iteration\n",
    "corpus = matutils.Sparse2Corpus(term_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2word = dict((v,k) for k, v in doc_term_vector.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-e512c3d93fce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# we can also run lsa on this:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# lsa = models.LsiModel(corpus=corpus, num_topics)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mlda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Python34\\Lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLdaState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m100.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpElogbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirichlet_expectation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msstats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[1;31m# if a training corpus was provided, start estimating the model right away\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python34\\Lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36mdirichlet_expectation\u001b[1;34m(alpha)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpsi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpsi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpsi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpsi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# keep the same precision as input\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "# we can also run lsa on this:\n",
    "# lsa = models.LsiModel(corpus=corpus, num_topics)\n",
    "\n",
    "# !!! skip this part:this take a really long time, hence I am going to pickle it and run later\n",
    "\n",
    "lda = models.LdaModel(corpus=corpus, num_topics=10, id2word=id2word, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-570af72f523a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlda\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lda' is not defined"
     ]
    }
   ],
   "source": [
    "lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lda_model.pkl',\n",
       " 'lda_model.pkl_01.npy',\n",
       " 'lda_model.pkl_02.npy',\n",
       " 'lda_model.pkl_03.npy',\n",
       " 'lda_model.pkl_04.npy',\n",
       " 'lda_model.pkl_05.npy']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !!! skip this part:this take a really long time, hence I am going to pickle it and run later\n",
    "\n",
    "joblib.dump(lda, 'lda_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_model = joblib.load('lda_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*think + 0.011*going + 0.008*im + 0.008*know + 0.007*obama + 0.006*dont + 0.005*look + 0.005*make + 0.004*people + 0.003*campaign'),\n",
       " (1,\n",
       "  '0.014*care + 0.013*health + 0.012*credit + 0.012*card + 0.011*credit card + 0.010*health care + 0.008*ive + 0.007*got + 0.006*seen + 0.006*making'),\n",
       " (2,\n",
       "  '0.005*let + 0.005*bad + 0.004*rules + 0.004*america + 0.004*god + 0.004*bush + 0.004*second + 0.004*opponent + 0.004*bless + 0.004*god bless'),\n",
       " (3,\n",
       "  '0.013*tax + 0.012*mccain + 0.010*families + 0.010*john + 0.008*john mccain + 0.007*years + 0.006*americans + 0.005*senator + 0.004*thats + 0.004*dont'),\n",
       " (4,\n",
       "  '0.008*make + 0.007*sure + 0.007*make sure + 0.005*need + 0.004*ill + 0.004*security + 0.004*lets + 0.004*help + 0.003*protect + 0.003*job'),\n",
       " (5,\n",
       "  '0.006*women + 0.005*work + 0.005*men + 0.005*america + 0.005*jobs + 0.004*men women + 0.003*young + 0.003*gas + 0.003*economy + 0.003*decisions'),\n",
       " (6,\n",
       "  '0.019*thats + 0.016*change + 0.015*american + 0.013*people + 0.009*need + 0.008*president + 0.008*washington + 0.007*american people + 0.007*election + 0.007*americans'),\n",
       " (7,\n",
       "  '0.015*states + 0.012*united + 0.012*united states + 0.010*america + 0.010*obama + 0.007*president + 0.007*right + 0.005*states america + 0.005*president united + 0.004*rate'),\n",
       " (8,\n",
       "  '0.014*street + 0.013*jobs + 0.012*new + 0.010*wall + 0.010*wall street + 0.007*economy + 0.006*energy + 0.006*crisis + 0.006*create + 0.004*war'),\n",
       " (9,\n",
       "  '0.009*insurance + 0.009*senator + 0.007*taxes + 0.007*companies + 0.006*mccain + 0.005*senator mccain + 0.005*billion + 0.004*year + 0.003*know + 0.003*clinton')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(num_words=10, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# note a document = a sentence\n",
    "# we want to find which sentences belong to which topic\n",
    "\n",
    "# !!! skip this part:this take a really long time, hence I am going to pickle it and run later\n",
    "\n",
    "lda_corpus = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !!! skip this part: already pickled\n",
    "lda_corpus_pickled = joblib.dump(lda_corpus, 'lda_corpus.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_corpus_unpickled = joblib.load('lda_corpus.pkl')\n",
    "#lda_docs = [doc for doc in lda_corpus_unpickled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-e6172b4b2c2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#!!!!!!!! don't run this already pickled below\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlda_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlda_corpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lda_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "#!!!!!!!! don't run this already pickled below\n",
    "lda_docs = [doc for doc in lda_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_docs_pickled = joblib.dump(lda_docs, 'model_data/lda_docs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_docs_unpickled = joblib.load('model_data/lda_docs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_doc_test = lda_docs_unpickled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30145"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getSentence(sentence):\n",
    "    return sorted(sentence, key=itemgetter(1), reverse=True)[0][0]\n",
    "\n",
    "sentenceAndTopicList = []\n",
    "for index, doc in enumerate(lda_docs_unpickled):\n",
    "    temp = [index, getSentence(doc)]\n",
    "    sentenceAndTopicList.append(temp)\n",
    "    \n",
    "sentenceAndTopicList[:20]\n",
    "len(sentenceAndTopicList)\n",
    "#getSentence(lda_doc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python34\\Lib\\site-packages\\ipykernel\\__main__.py:8: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docIndex</th>\n",
       "      <th>topic</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26476</th>\n",
       "      <td>26476</td>\n",
       "      <td>1</td>\n",
       "      <td>I will reform our health care system so we ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24201</th>\n",
       "      <td>24201</td>\n",
       "      <td>1</td>\n",
       "      <td>In the Senate, I worked across the aisle to c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26477</th>\n",
       "      <td>26477</td>\n",
       "      <td>1</td>\n",
       "      <td>If you have health care, my plan will lower y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>1828</td>\n",
       "      <td>1</td>\n",
       "      <td>Other nations would feel great pressure to ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16984</th>\n",
       "      <td>16984</td>\n",
       "      <td>1</td>\n",
       "      <td>And, you know, I've seen and heard worse</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       docIndex  topic                                           sentence\n",
       "26476     26476      1   I will reform our health care system so we ca...\n",
       "24201     24201      1   In the Senate, I worked across the aisle to c...\n",
       "26477     26477      1   If you have health care, my plan will lower y...\n",
       "1828       1828      1   Other nations would feel great pressure to ac...\n",
       "16984     16984      1           And, you know, I've seen and heard worse"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getSentenceFromIndex(data, textList):\n",
    "    return textList[data]\n",
    "\n",
    "df = pd.DataFrame(sentenceAndTopicList)\n",
    "df.rename(columns={0: 'docIndex', 1: 'topic'}, inplace=True)\n",
    "#df['sentence'] = df.apply(lambda row: getSentenceFromIndex(row.docIndex, updatedTextList), axis=1)\n",
    "df['sentence'] = df.apply(lambda row: getSentenceFromIndex(row.docIndex, txtList), axis=1)\n",
    "df.sort(['topic'], ascending=True, inplace=True)\n",
    "df[df['topic']==1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' I will reform our health care system so we can relieve families, businesses, and our economy from the crushing cost of health care by investing in new technology and preventative care',\n",
       " ' In the Senate, I worked across the aisle to crack down on these schemes',\n",
       " ' If you have health care, my plan will lower your premiums',\n",
       " ' Other nations would feel great pressure to accommodate Iranian demands',\n",
       " \" And, you know, I've seen and heard worse\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic1 = []\n",
    "df_topic1 = df[df['topic'] == 1]\n",
    "topic1_sentences = list(df_topic1.iloc[:,2].values)\n",
    "\n",
    "topic1_sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This part is taken from markov.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take previous section and add 'START HERE' and 'STOP NOW' at the beginning and ending of each list element respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It's\",\n",
       " 'a',\n",
       " 'philosophy',\n",
       " 'that',\n",
       " 'says',\n",
       " 'we',\n",
       " 'have',\n",
       " 'to',\n",
       " 'choose',\n",
       " 'between',\n",
       " 'a',\n",
       " 'government-run',\n",
       " 'health',\n",
       " 'care',\n",
       " 'plan',\n",
       " 'that',\n",
       " 'would',\n",
       " 'do',\n",
       " 'exactly',\n",
       " 'that']"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======= (4) Start doing n-grams\n",
    "# will apply above strategy to sentences \n",
    "#docList = topic1_sentences\n",
    "docList = ['START HERE ' + i + ' STOP NOW' for i in topic1_sentences]\n",
    "\n",
    "def find_ngrams(input_string, n):\n",
    "    # replace double space by single space\n",
    "    input_string = input_string.replace('  ', ' ')\n",
    "    input_list = input_string.split(' ')\n",
    "    return zip(*[input_list[i:] for i in range(n)])\n",
    "\n",
    "# ======= (5) Start doing n-grams\n",
    "\n",
    "def generateNGramDict(docList):\n",
    "    d = defaultdict(list)\n",
    "    for doc in docList:\n",
    "        trigrams = list(find_ngrams(doc,3))\n",
    "        for trigram in trigrams:\n",
    "            d[trigram[:2]].append(trigram[2])\n",
    "            \n",
    "    return d\n",
    "\n",
    "dtemp = generateNGramDict(docList)\n",
    "dtemp\n",
    "\n",
    "# ======= (6) Test sentence generator\n",
    "\n",
    "def generateText(triGramDict, firstWord='START', secondWord='HERE'):\n",
    "    newSpeech = [firstWord,secondWord]\n",
    "    \n",
    "    while secondWord != 'NOW':\n",
    "        firstWord, secondWord = secondWord, np.random.choice(triGramDict[(firstWord, secondWord)])\n",
    "        newSpeech.append(secondWord)\n",
    "        \n",
    "    return newSpeech[2:-2]#' '.join(newSpeech[2:-2])\n",
    "\n",
    "#generateText(dtemp, 10, 'Ive', 'got')\n",
    "generateText(dtemp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## How about making dictionary of all the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' I will reform our health care system so we can relieve families, businesses, and our economy from the crushing cost of health care by investing in new technology and preventative care',\n",
       " ' In the Senate, I worked across the aisle to crack down on these schemes',\n",
       " ' If you have health care, my plan will lower your premiums',\n",
       " ' Other nations would feel great pressure to accommodate Iranian demands',\n",
       " \" And, you know, I've seen and heard worse\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicDict = {}\n",
    "\n",
    "numOfTopics = 10\n",
    "for i in range(numOfTopics):\n",
    "    df_topic = df[df['topic'] == 1]\n",
    "    topic_sentences = list(df_topic.iloc[:,2].values)\n",
    "\n",
    "    topicDict[i] = topic_sentences\n",
    "    \n",
    "topicDict[2][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convertCharToLower(numOfTopics):\n",
    "    for i in range(numOfTopics):\n",
    "        topicDict[i] = [i.lower().strip() for i in topicDict[i]]\n",
    "        \n",
    "def putSentenceTags(numOfTopics):\n",
    "    for i in range(numOfTopics):\n",
    "        topicDict[i] = ['START HERE ' + i + ' STOP NOW' for i in topicDict[i]]\n",
    "\n",
    "convertCharToLower(10)\n",
    "putSentenceTags(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['START HERE i will reform our health care system so we can relieve families, businesses, and our economy from the crushing cost of health care by investing in new technology and preventative care STOP NOW',\n",
       " 'START HERE in the senate, i worked across the aisle to crack down on these schemes STOP NOW',\n",
       " 'START HERE if you have health care, my plan will lower your premiums STOP NOW',\n",
       " 'START HERE other nations would feel great pressure to accommodate iranian demands STOP NOW',\n",
       " \"START HERE and, you know, i've seen and heard worse STOP NOW\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicDict[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topicDict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4d0f247b6bb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#docList = topic1_sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mchooseTopic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdocList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'START HERE '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' STOP NOW'\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtopicDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchooseTopic\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfind_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'topicDict' is not defined"
     ]
    }
   ],
   "source": [
    "# ======= (4) Start doing n-grams\n",
    "# will apply above strategy to sentences \n",
    "#docList = topic1_sentences\n",
    "chooseTopic = 8\n",
    "docList = ['START HERE ' + i + ' STOP NOW' for i in topicDict[chooseTopic]]\n",
    "\n",
    "def find_ngrams(input_string, n):\n",
    "    # replace double space by single space\n",
    "    input_string = input_string.replace('  ', ' ')\n",
    "    input_list = input_string.split(' ')\n",
    "    return zip(*[input_list[i:] for i in range(n)])\n",
    "\n",
    "# ======= (5) Start doing n-grams\n",
    "\n",
    "def generateNGramDict(docList):\n",
    "    d = defaultdict(list)\n",
    "    for doc in docList:\n",
    "        trigrams = list(find_ngrams(doc,3))\n",
    "        for trigram in trigrams:\n",
    "            d[trigram[:2]].append(trigram[2])\n",
    "            \n",
    "    return d\n",
    "\n",
    "# nGramDict has key=topic, value = default \n",
    "# nGramDict = {}\n",
    "# for i in range()\n",
    "dtemp = generateNGramDict(docList)\n",
    "dtemp\n",
    "\n",
    "# ======= (6) Test sentence generator\n",
    "\n",
    "def generateText(triGramDict, firstWord='START', secondWord='HERE'):\n",
    "    newSpeech = [firstWord,secondWord]\n",
    "    \n",
    "    while secondWord != 'NOW':\n",
    "        firstWord, secondWord = secondWord, np.random.choice(triGramDict[(firstWord, secondWord)])\n",
    "        newSpeech.append(secondWord)\n",
    "        \n",
    "    return newSpeech[2:-2]#' '.join(newSpeech[2:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generateText' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1d5da641c064>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0msentence_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerateText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0msentence_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerateText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0munique_words_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generateText' is not defined"
     ]
    }
   ],
   "source": [
    "# ======= (7) Generate pair of sentences\n",
    "result = True\n",
    "while result:\n",
    "    sentence_1 = generateText(dtemp)\n",
    "    sentence_2 = generateText(dtemp)\n",
    "    unique_words_1 = set(sentence_1)\n",
    "    unique_words_2 = set(sentence_2)\n",
    "    if len(unique_words_1.intersection(unique_words_2)) > 10:\n",
    "    #if (len(unique_words_1.intersection(unique_words_2))) / (len(unique_words_1.union(unique_words_2))) > 0.3:\n",
    "        print(' '.join(sentence_1[2:-2]))\n",
    "        print('------------------')\n",
    "        print(' '.join(sentence_2[2,-2]))\n",
    "        result = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30145x3000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 243034 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,2), max_features=3000, min_df = 1, stop_words = 'english')\n",
    "dtm = vectorizer.fit_transform(txtList) \n",
    "pd.DataFrame(dtm.toarray(), index=txtList, columns=vectorizer.get_feature_names()).head(10)\n",
    "\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(100, algorithm = 'arpack')\n",
    "dtm_lsa = lsa.fit_transform(dtm)\n",
    "dtm_lsa = Normalizer(copy=False).fit_transform(dtm_lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEDCAYAAAD5kUlYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8XGd97/HPzGjfZWss2bJsObH885bFWWyHhIZsYDsQ\n03IhBG7JQoshpPSW0gJtKXB7byGX2zaEFG5KSG6AS5JCWBwSCCEQsuLEiWM7sfM4si3bki1LtrXv\ny9w/ztijCEs6tmY80sz3/XrpZZ2Z5znzm5+l+ek55znPCUQiEUREROIpmOwAREQk9ai4iIhI3Km4\niIhI3Km4iIhI3Km4iIhI3Km4iIhI3GUkcudmtga4AwgB9zjnbj9JmzuBtUA3cJNzbkv08XuBa4Em\n59w5J+n318DXgDLn3LHEvQsRETlVCRu5mFkIuAtYAywFbjCzJaParAMWOudqgI8B3xrx9H3Rvifb\ndxVwDbAvAaGLiMgkJfKw2Eqg1jlX55wbAB4E1o9qcx1wP4BzbhNQYmYV0e1ngJYx9v2vwN8mJGoR\nEZm0RBaXSuDAiO366GOn2uYtzGw9UO+c2xaPIEVEJP4Sec7F77oyAb/9zCwP+Du8Q2Jj9RcRkSRL\nZHFpAKpGbFfhjUzGazM3+thYzgaqga1mdrz9y2a20jnXNFanSCQSCQRUg0RETtFpf3AmsrhsBmrM\nrBo4CFwP3DCqzUbgNuBBM1sNtDrnDo+1Q+fcdqD8+LaZ7QUunGi2WCAQoLm547TeRKoJhwuViyjl\nIka5iFEuYsLhwtPum7BzLs65QbzC8TiwA3jIObfTzDaY2YZom8eAPWZWC9wN3Hq8v5k9ADwPLDKz\nA2Z280leRks6i4hMQYE0WXI/or9EPPqrLEa5iFEuYpSLmHC48LQPi+kKfRERiTsVFxERiTsVFxER\niTsVFxERiTsVFxERiTsVFxERiTsVFxERiTsVFxERiTsVFxERiTsVFxERiTsVFxERiTsVFxERiTsV\nFxERiTsVFxERiTsVFxERiTsVFxERiTsVFxERiTsVFxERiTsVFxERiTsVFxERiTsVFxERiTsVFxER\niTsVFxERibuMRL+Ama0B7gBCwD3OudtP0uZOYC3QDdzknNsSffxe4FqgyTl3zoj2XwPeDfQDu4Gb\nnXNtiX4vIiLiT0JHLmYWAu4C1gBLgRvMbMmoNuuAhc65GuBjwLdGPH1ftO9ovwKWOefOA3YBnx8v\njv2N7QwMDp32+xARkVOT6JHLSqDWOVcHYGYPAuuBnSPaXAfcD+Cc22RmJWZW4ZxrdM49Y2bVo3fq\nnHtixOYm4H3jBfHJr/2WAFBalM2KhWE+/M5Fk3lPIiIygUSfc6kEDozYro8+dqptxnML8Nh4Dd61\nej42r4Tu3kGefKWe7t6BU9i9iIicqkSPXCI+2wVOp5+Z/T3Q75z7wXjtbnv/+QDc98jr/PipWjoH\nIsyvKvQZWuoJh9P3vY+mXMQoFzHKxeQlurg0AFUjtqvwRibjtZkbfWxcZnYTsA64yk8gzc0dzCzI\nAmD7riZmFWb56ZZywuFCmps7kh3GlKBcxCgXMcpFzGSKbKIPi20Gasys2syygOuBjaPabAQ+AmBm\nq4FW59zh8XYanYH2N8B651yv32DmzioA4EBTp+83ICIipy6hxcU5NwjcBjwO7AAecs7tNLMNZrYh\n2uYxYI+Z1QJ3A7ce729mDwDPA4vM7ICZ3Rx96htAAfCEmW0xs2/6iWf2zDxCwQD1zSouIiKJFIhE\n/J4WmdYix4e5//idTTS19vDNT19OMDD6VE/q05A/RrmIUS5ilIuYcLjwtD8k0+4K/apZBfQPDNPc\n2pPsUEREUlbaFZfj513qdd5FRCRh0q64VIV1Ul9EJNHSrrhoxpiISOKlXXEpzs+iIDdTM8ZERBIo\n7YpLIBCgalYBza299PQNJjscEZGUlHbFBbwZYwANR7qSHImISGpKy+IyVyf1RUQSKi2LS5WmI4uI\nJFRaFpc5ZXkEAnBAJ/VFRBIiLYtLZkaIihl51Dd1kibL34iInFFpWVzAOzTW2z/E0TbfiyqLiIhP\naV1cAOoatUCdiEi8pW1xsXmlAGzfczTJkYiIpJ60LS5nzS6iIDeTbXuO6ryLiEicpW1xCQYDnHPW\nTNo6+9l/WLPGRETiKW2LC8B5C2cCsHX3kSRHIiKSWtK6uCxfMINgIMDWWp13ERGJp7QuLnk5mdTM\nLabuUDvtXf3JDkdEJGWkdXEBOHfhTCJo1piISDypuJxdBsDW3SouIiLxkvbFZc7MPMqKc3h971EG\nh4aTHY6ISEpI++ISCAQ47+wyevqGqK1vS3Y4IiIpISOROzezNcAdQAi4xzl3+0na3AmsBbqBm5xz\nW6KP3wtcCzQ5584Z0X4G8BAwH6gDPuCca51MnOcunMmTr9SzdfcRFs8vncyuRESEBI5czCwE3AWs\nAZYCN5jZklFt1gELnXM1wMeAb414+r5o39E+BzzhnFsEPBndnpTF80rIyQrxsmvW1foiInGQyMNi\nK4Fa51ydc24AeBBYP6rNdcD9AM65TUCJmVVEt58BWk6y3xN9ov++d7KBZmaEOL+mjCNtvVrIUkQk\nDhJZXCqBAyO266OPnWqb0cqdc4ej3x8GyicT5HErF3u7eXHn4QlaiojIRBJ5zsXv8aXAafbDORcx\nM1/tw+HCcZ9/R2ke9zy6g5d3HeGTH1hBIDA6rNQxUS7SiXIRo1zEKBeTl8ji0gBUjdiuwhuZjNdm\nbvSx8Rw2swrnXKOZzQaa/ATT3Dzx4a7zF5bx/GuN/H5rAwsri/3sdtoJhwt95SIdKBcxykWMchEz\nmSKbyMNim4EaM6s2syzgemDjqDYbgY8AmNlqoHXEIa+xbARujH5/I/DTeAW8csksAF7a6ateiYjI\nGBJWXJxzg8BtwOPADuAh59xOM9tgZhuibR4D9phZLXA3cOvx/mb2APA8sMjMDpjZzdGnvgpcY2a7\ngCuj23GxtHoGedkZbHZNDGvWmIjIaQukydTbiN9h7r2P7uTZ7Yf43IcvYFFVSYLDOvM05I9RLmKU\nixjlIiYcLjztk89pf4X+aCcOjb2hQ2MiIqdLxWWUxfNLyc/JYPMbTVprTETkNKm4jJIRCnLpObNp\n6+rnqS0TTVwTEZGTUXE5iXWXzCcnK8TG5+ro6RtMdjgiItOOistJFOVlsXbVPDp7Bvjlpv3JDkdE\nZNpRcRnDOy+eR3F+Fo+/tJ/Wzr5khyMiMq2ouIwhOyvE+ssW0D8wzMbn6pIdjojItOKruJhZgZmd\na2ZBM8tPdFBTxdvPm035jDyefvUgjce6kx2OiMi0MWFxMbOrgFfxll0pB+rM7F2JDmwqCAWD/PHb\nFzAcifDbVzRzTETELz8jl68AbwdanHOHgHcAX0tkUFPJBYvCFOZl8sLrjbruRUTEJz/FJRgtKgA4\n517nFJbFn+4yQkEuWVZBZ88AW2uPJDscEZFpwU9xOWBm7wEwsxIz+3sgrebnXnbubACe2XZogpYi\nIgL+isvHgQ/j3XdlD7AC7373aWNuuIAFswvZvucoLR2aliwiMpEJbxYWvb/KB89ALFPaZefMZu+h\nDl54vZF1q+cnOxwRkSltwuJiZntP8nDEOXdWAuKZslYtLefB39TyzLZDrF01L6VvgywiMll+bnN8\nxYjvM4H3AjmJCWfqysvJ5MJFYX6/4zC7G9pZODc1b4MsIhIPfg6L1Y166Gtm9jLwTwmJaAq79NzZ\n/H7HYZ7ZdlDFRURkHH4Oi11ObOpxAFhOGo5cAJbML2VmUTYvvdHEh65ZRHZmKNkhiYhMSX4Oi32Z\nWHGJAEeAGxMW0RQWDAS4ZHkFP39+H6/sauaSZRXJDklEZEryc1jsHWcgjmnjbctn8/Pn9/H89kMq\nLiIiYxizuJjZb8fpF3HOXZmAeKa8ihl5nF1ZxI66Fo619zKjKC2PEIqIjGu8kcuXx3kubZZ/OZlL\nl89md0M7L7zeyLWXVCc7HBGRKWfMK/Sdc08d/wLagSFgONrn7DMT3tS0csksMkJBnn+tkUgkreus\niMhJ+Zkt9l3gEmAmsAM4H3gOuNdH3zXAHUAIuMc5d/tJ2twJrAW6gZucc1vG62tm5wP/B8gGBoFb\nnXMvTfhO4ygvJ5MLFpXx4s4m9h7q4Kw5RWfy5UVEpjw/a4v9EbAM+CGwAViF98E+LjMLAXcBa4Cl\nwA1mtmRUm3XAQudcDd56Zd/y0fd/AV90zq0A/jG6fca9bbm3mOVz27WYpYjIaH6Ky0HnXD+wEzg3\nuuR+oY9+K4Fa51ydc24AeBBYP6rNdcD9AM65TUCJmVVM0HcYOH4FYwmQlLt4LVtQSnF+Fi/uPExX\n70AyQhARmbL8FJcGM/s88DywwcxuAPzc6rgSODBiuz76mJ82c8bp+9/wVgnYj3fTss/7iCXuQsEg\nV180l67eQb77S6dzLyIiI/i5iPIW4Frn3Itm9jDeCsmf8NHP76ftqa4AeSvw35xzPzGz9+Od+7lm\nok7hsJ/B1qn502uXsWNfKy+90cRlKyq58qJ5cX+NREhELqYr5SJGuYhRLibPT3H5H8D3AJxz3wC+\n4XPfDXj3gDmuCm8EMl6budE2meP0/Yhz7lPR738E3OMnmObmDn9Rn6Kb1hhfuu9FvvnwNsqLsplV\nmpeQ14mXcLgwYbmYbpSLGOUiRrmImUyR9XNY7E3gDjPbaWb/YGbVPve9Gagxs2ozywKuBzaOarMR\n+AiAma0GWqP3jxmv78HoemcAVwK7fMaTEOGSXP7rO42+/iG+/cgOhoaHkxmOiMiUMGFxcc7d5Zy7\nDG/mVi/wMzN71ke/QeA24HG8KcwPOed2mtkGM9sQbfMYsMfMaoG78Q55jdk3uus/B/7FzF7FG1Ul\n/a6YlyyrYPXScnYfbOfHv9uT7HBERJIu4OdEtJkVA/8FbwQxB+/DfjotuR9J9DC3u3eQf7r/JQ63\n9HDre5dz0eJZCX2906Uhf4xyEaNcxCgXMeFw4WnfFXHCkYuZPULs4skvOOeWT7PCckbk5WRw25+c\nQ3ZmiO88upOG5s5khyQikjR+zrl8G5jvnPuL6LUoMobKcAEfvXYJfQND3PXj7XT3DiY7JBGRpPAz\nW2wb8FUzm0Fs2nDEOXdL4sKavi5aPIu1q+bxi037+b+/2Mmtf3xOskMSETnj/BSX/wSejn4dpysG\nx/Enl59FbUMbm10zW3Y1s2JRONkhiYicUX6KS4Zz7jMJjySFhIJBblyzmC/e+yLff2IXi+eXkpvt\nJ9UiIqnBzzmXZ83suuj1JuLTnLJ8rr1kPi0dffzkaU1PFpH04qe4vB/4KdBrZsPRr6EEx5USrr1k\nPuUz8njy5Xr2HmpPdjgiImfMhMdqnHOzz0QgqSgzI8SN7zL+1wNbuP8Xb/APN15ERshPPRcRmd78\n3CwsG/gMYMBfAH8JfDW6DL9MYPH8Ui47dzbPbjvET57ew/uvWJjskEREEs7Pn9H/DhQAF+Ld+bEG\n+E4ig0o1N1xVw6zSXH6xaT+v7Tma7HBERBLOT3G50Dn3eaDfOdeFt9DkBYkNK7XkZmfw8fXLCAUD\n3PPzHbR19iU7JBGRhPJTXIZHzRQrw7sbpJyC6ooi3n/FQtq7B/j2z3cwrJuLiUgK81Ncvg78Gqgw\ns68DLwN3JDSqFHXNRXM57+yZ7Khr4fEX9yc7HBGRhPGz5P538e48+T+B3cC7nXM653IaAoEAt1y7\nhKK8TH7y9B4tbikiKcvPqsg34p3M7wDagPPM7L+Y2fJEB5eKCvOyuHHtYgaHItzz6E4Gh3SEUURS\nj5/DYtcBXwLOi359Ae9GXveZ2acTF1rqWlET5tLlFexr7OCx3+9LdjgiInHnp7jMBi5wzn3aOfdp\n4KJov7cBNyUwtpR2w9U1lBZm88hzdexr1I2JRCS1+CkuZcDIkwM9wAzn3ACaNXba8nIyuXndYoaG\nI/z7T7ZzuKU72SGJiMSNn+LyMPAbM/ukmX0Kb+bYT8zsI8ChhEaX4pYvmMkfv30BR9p6+cr3XtYI\nRkRShp/ZYp8HvgYsAqrxln75ArAL+FBCo0sD77l0AX/6zkV0dA9w+w9eYWfdsWSHJCIyaWMWFzO7\nIPrv5UA78GPgZ0C3mf2Rc+73zrmWMxNmarvigrl8/L3LGRwa5t9+uI39hzWCEZHpbbyFKz8B/Dnw\nZU5+58krEhJRmrp48SwyQgG+8fB27n10p1ZQFpFpbczi4pz78+i/7zhj0aS5FTVh/ui82Ty99RA/\nf76O9779rGSHJCJyWvwsuV8NfBtYALwd+AFwi3Nur4++a/CWigkB9zjnbj9JmzuBtUA3cJNzbstE\nfc3sL4BbgSHgUefcZyeKZbq4/soaXt97jEdf2MeKmjDzKwqTHZKIyCnzc9zlbuB/412hfxivuNw/\nUSczCwF3AWuApcANZrZkVJt1wELnXA3wMeBbE/U1syvwLuw81zm3PBpbysjNzuCmdUsYGo5wz6M7\nGBjUbG8RmX58XefinHscwDk37Jz7NlDso99KoNY5Vxe9JuZBYP2oNtcRLVTOuU1AiZlVTND3E8BX\noo/jnGv2Ecu0sqx6BlesqKShuYsv3fciT21poG9Ad5YWkeljwsNieLPD5h7fMLPLgF4f/SqBAyO2\n64FVPtpUAnPG6VsD/JGZ/XM0js845zb7iGda+cAVCxkYGuaF1xr57uOOHz+9h1VLyrF5JSyqKqEo\nP2vinYiIJImf4vJp4FHgLDPbCswA3u+jn98blgR8tjsuAyh1zq02s4uB/wQmPPMdDk+/cxefvXEl\nx9p7eey5vTz2fB1PvlLPk6/UA2DzSvmHW1ZRUph9yvudjrlIFOUiRrmIUS4mb8Li4px7Kfohvgjv\n5Pobzjk/t1JsAKpGbFfhjUDGazM32iZznL71eNfcHI9t2MxmOufGvX9wc/P0vXbkXRfN5crz57D3\nUDu7DrTy2t5juP0t/MePt3LzuiUT72CEcLhwWucinpSLGOUiRrmImUyR9TNywTnXD7x2ivveDNRE\nZ5sdBK4HbhjVZiPeCssPmtlqoNU5d9jMjo7T96fAlcDvzGwRkDVRYUkFmRlBFlV5h8TWrp7Hl+97\niWe3HeIdKypZMLso2eGJiLxFwq7Sc84N4hWOx4EdwEPOuZ1mtsHMNkTbPAbsMbNavFlpt47XN7rr\ne/EO0W0HHgA+kqj3MFWFgkE+fM0iIsD/e2KXbpksIlNOIDLBB5OZzXDOHRv12Hzn3HS6EUkkFYe5\n/+dnr/HiziY+eu0SLj1ntq8+GvLHKBcxykWMchETDhee6jnxE8Y8LGZmVXgjm0ej16Mclwk8Btjp\nvqjExweuWMirtUf44VO7qZlbTFF+FtmZIQKB0/55EBGJi/HOufx34B1404J/N+LxQeDnCYxJfJpR\nlMO1l1Tzk6f38Lm7fw9AMBCgrCSHxfNKWDyvFJtXSulpzCgTEZmM8dYWuxnAzD57smVbZGpYs3Ie\nAIeOdNHdN0hX7wAHj3Tx9NZDPL3Vu91OSUEW1RVFVFcUsv6KmmSGKyJpws9ssZsBFZcpKjMjyHve\nVv2Wx4aHI+w73MEb+1t480AbdY3tvFp7hFdrj/DU1oN86n3nUF2hGWYikjh+Tug/DGwFNuHd4jgA\nRJxzTyc+vLhJyRP6p6K1s48XXm/kR0/tJjMjyMfXL+f8hWXJDiupdOI2RrmIUS5iEnJCf4SZePdu\nGX3/Ft3PZRopKchm7ar51Myfyf/+/ma+8fA2PnT1Iq68oFITAEQk7iYcuaSItB+5HBcOF7JpawN3\n/mgr7d0DXLx4FjeuWUxejq/raVOK/kKNUS5ilIuYhI5czOy3eOuEHX+R49Xo+OGxK0/3xSU5zppT\nxBduvJi7H3mdl95ooq6xnY+vX64r/UUkbvxcof8yUAf8HfC3eOdfjgJfwrsFskxDM4tz+OyHVnDt\nJfM50trLP3/vZR59oY7h4bQYyYpIgvk5FnK5c+7iEdsvmdlm59zvxuwh00IoGOR9l5/N4nml3PPo\nDh7+3R627j7Kn717KbNKcpMdnohMY35GLjlmtvT4hpmdj3d7YUkRyxbM4J8+uoqLFs+itr6NL977\nIvf/8g1+80o9b9a30tev/24ROTV+Ri6fAX5jZg1451kKgQ8mNCo54wpyM/nE+mX8vqaMHzyxi9+9\nevDEc1kZQS6wMJeeM5sl80sJanaZiEzA12wxM8sBzgG6ARddtXg60WyxKD8zYQYGhzl0tIsDTZ3s\nP9zJ1tojNLX2AFBWnMOn3ncuc2cVnIlwE0qzgmKUixjlImYys8X8XES5CrgU+HfgEWAF8Ann3I9O\n90WTQMUl6nR+cSKRCLUNbTyz7RDPbjtE+Yw8/vHGi8jNnt7Tl/UhEqNcxCgXMZMpLn7OudyJN2Ps\nfXhX6F8IfO50X1Cmn0AgQM3cEm5Zt4Q1K+dx+Fg39//yDdLkGikROQ1+ikswOjPsWuBh59x+vNsd\nSxr6k8vPYmFlMS/ubOKpEedlRERG8lNcus3sM8BVwM/N7C8BjRnTVEYoyMfXL6MgN5MHfv0mew+1\nJzskEZmC/BSXDwN5wJ9E70g5B/hQQqOSKW1GUQ5/9u6lDA4N85Xvv8Ijz+1lcGg42WGJyBSitcXS\nTDxPVm7Z1cx3f+Vo6+ynMpzPTWsWc3ZlcVz2fSboxG2MchGjXMQk+oS+yEmtWBTmf/7ZKi4/fw4N\nzV388/df5qfP7GFoWKMYkXSn4iKTkpeTyY1rFvPZD61gRmE2G5+r46vff4Wmlu5khyYiSaTiInFh\n80r58i0rWb20nN0H2/nivS/x7Ud2sGnHYbp6B5IdnoicYdP7KjiZUvJyMvnYdcs45+yZ/PC3tbzw\neiMvvN5IMBBgTlk+c8ryqCzLZ1FVCTavNNnhikgCJbS4mNka4A6862Lucc7dfpI2dwJr8ZaWuck5\nt8VPXzP7a+BrQFl0FptMEZcsq2DV0nIOHO5k2+4jbN9zjP1NHdQ3d55oc92l1ay/bIHugimSohJW\nXMwsBNwFXA004C3Vv9E5t3NEm3XAQudcTXSZmW8Bqyfqa2ZVwDXAvkTFL5MTDASYX1HI/IpC3nPp\nAoYjEY6191Lf1MUDT+5i43N1HG3r5ca1i8kI6eisSKpJ5MhlJVDrnKsDMLMHgfXAzhFtrgPuB3DO\nbTKzEjOrABZM0Pdf8W5c9rMExi9xFAwEKCvOpaw4l7PmFPH1H23judcaOdrey8ol5WRlBsnODLGo\nqoTCvKxkhysik5TI4lIJHBixXQ+s8tGmEu9CzZP2NbP1QL1zbpuZxTtmOQOK8rP42w+t4D82vs6W\nN4/wxv7WE8+VFGTx2Q9fQHlpXhIjFJHJSmRx8Xt1pu+D7maWi3e75WtOtX84XOj3ZVLeVMnFFz/2\nNrbXNtPa2U9f/xAHDnfws6d38y8PvspXPnkZFTPzEx7DVMnFVKBcxCgXk5fI4tIAVI3YrsIbgYzX\nZm60TeYYfc8GqoGt0VHLXOBlM1vpnGsaLxhdceuZalcfV5bmUlnq3VL5grNnkBWCH/52N5+761k+\n++EVlBUn7nbLUy0XyaRcxCgXMZMpsoksLpuBGjOrBg4C1wM3jGqzEbgNeNDMVgOtzrnDZnb0ZH2j\nJ/TLj3c2s73AhZotljrWrprP0FCEHz+9hy9850Vys0JEIpARCvDut1Vz+fmVyQ5RRHxI2DSd6N0q\nbwMeB3YADznndprZBjPbEG3zGLDHzGqBu4Fbx+t7kpdJi4XR0s2731bNB65YSHF+FlmZIXKyM+js\nHeT+Xzoe+s2bDA/rv11kqtPClWlmug75m1q6+fqPtnHoaDcrasr42HuWkZ01udsKTddcJIJyEaNc\nxCT0NscpQsUlajr/4nT1DvDNn7zGzn0tBAMBSguzmVmUzazSPBbPL2HJ/BmUFmb73t90zkW8KRcx\nykXMZIqLln+RaSM/J5O/+sB5/OzZvbj9rRxt7+XNhjZ21bfx7PZDAMwpy2dpdSnLF8zE5pWQnamb\npookg4qLTCsZoSDvu/zsE9uDQ8McPNLFzn0t7KhrwR1o4debu/j15noyQkGqZxdSWZbPnLJ85pcX\nUjO3WEvOiJwBKi4yrWWEgswrL2ReeSHvWjmPgcFhautbeW3vMV7be4w9De3U1redaD+nLJ93XVzF\n6mUVSYxaJPXpnEuaSbfjyQODwzQe66bhSCfbdh/lpZ1NDA1HKMrP4gKbRVlR9omRzYyibELB9Fzn\nLN1+LsajXMTohP7EVFyi0v0X51h7L79+uZ6nXz1Id9/gW54LBQOUFedQGS7gukurmVeePldpp/vP\nxUjKRYyKy8RUXKL0i+MZjkQYDobY5g5T39zJ4WPdNLX0cLilh86eAQIBuOaiKtZftoDc7NQ/eqyf\nixjlIkazxUROUTAQoLwsn4xImAsWhd/y3Gt7j/L9x3fxq5cO8NIbTVxkswiX5FBWksu8WQXMKMpJ\nUtQi04eKi8goyxfM5L9/dCWPvrCPX2zaxxObD7zl+bMri1i5pJyLbBYlBVmafSZyEjoslmY05I/x\nk4vu3gEaj/VwpK2H5tYedtS18Mb+Fo7/2mRmBCnOz6I4P4tF80q4dPls5pQlfjXneNPPRYxyEaNz\nLhNTcYnSL07M6eairbOPza6Z1/ceo62rj9bOftq7+hmKrnm2YHYhFy8uZ8Fsb4r0dDhno5+LGOUi\nRudcRM6g4oJsrrpwLlddOPfEYwODQ2x58wjPbW/ktb1H2XvI+3AKABUz87B5pSydX8ri+aUU5GYm\nKXKRM0fFRSQOMjNCrFxSzsol5bR29uH2t7KvsYO6xnbqGjt4aksDT21pIACUFmVTWpBNSWE24ZJc\nFswu4qzZRcwoytb5G0kZKi4icVZSkM2qpeWsWurdemhoeJi6Qx3sqDvGzn0tNLf2UNfYwdDB9rf0\nK87P4tyzZ3KhzWJpdSkZofS8oFNSg865pBkdT45JZi6GIxE6ugc4dKSLvYfa2XOonTfr22jv6gcg\nNzuDs2YXEi7NI1ySQ3lpHnPD+ZSV5BJMwOhGPxcxykWMzrmITDPBQODELLPF80sBGB6OsPtgGy+7\nZl7Z1cx9JzMlAAAL+klEQVTrdS1Q1/KWftmZISrD+cwqyaWsJIey4lxKCrLIz8kkPzeTwrxM8rIz\ndHhNkk7FRWSKCAYD1MwtoWZuCR+8qobe/kGOtPbS1NrDoaNdNDR3Ud/cyb7GDvaMOqQ2UnZWiLKi\nHMqKc7h4ySxWLinXITY541RcRKaonKwM5s4qYO6sAiC2isDQ8DAt7X0caeulua2H9q5+unoH6ewZ\noL2rn2PtvRxt76PhSBdbdx/lh0/t5uoL53Lx4lnMLM5J28U55czSOZc0o+PJMamei+bWHp58uZ6n\ntx6kt38I8BbnnFmUQ0lhNqGgd+gsGIDCgmxCQE52Bvk5GRTlZ1GUl0VxQRazSnIpyk+flQhS/efi\nVOici4j8gXBJLh+8qob1ly3g2e2H2HuonebWHppbeth1oOeU9pWdFaK8JJd55YWcXVnE2ZXFzJmZ\nTzCYHgVHTp2Ki0iKy83O4JqLqt7y2HAkAhHv30gECotzOdDQSk/fIF09A7R199PRNUBLZx/N0dWi\nG491s7+p88QtpQPERjoFuZmES3KZVep9zSjMoTg/i6KCLApyMxMyw02mNhUXkTQUDAQgAEG8D/3C\nvCzCJbnj9hkejtBwpIvdDW3sbmijua2X7t4BunoHqW/uoq7x5IeSMkIBZhTmMKMom5lFOZQW5TCz\nKJsZRTknZswV5mVpFJRiVFxExJdgMEDVrAKqZhXwjhWVb3luOBKhtaOPppYemlp7aO3so62zn9ZO\nb+21Y+29vLG/dcx9Hx8FhYIBQqEAGcEAoWCQUChAKBiguCCbitI8KmbmMbMoh9zsELnZGeRlZ1Bc\nkEVmRijB715OVcKLi5mtAe4AQsA9zrnbT9LmTmAt0A3c5JzbMl5fM/sa8G6gH9gN3Oycaxu9XxE5\nM4KBADOKcphRlHPiup3RBgaHOdbRy7H2Po6193KsvZe2Lm/Rz/aufrr7hhgaHmZoOMLQUIT+wSGG\n+yMMDkWob+7i9b3Hxnz9wrxMSguyKSrIojA3i8K8TIoLsggX5xIu8b5ys0NpMylhKkhocTGzEHAX\ncDXQALxkZhudcztHtFkHLHTO1ZjZKuBbwOoJ+v4K+KxzbtjMvgp8HvhcIt+LiExOZkaQ8tI8ykvz\nTrlvT98gh1u6aTzaTUtHHz39Q/T2DdLVO0hrZx8tHX00tnjnhMYSCEBuVga52SHyczIpyMukIDeT\nkoJs5pTlU1mWPy1vlzBVJXrkshKodc7VAZjZg8B6YOeINtcB9wM45zaZWYmZVQALxurrnHtiRP9N\nwPsS/D5EJIlyszOoriiiuqJo3HZ9/UN0dPfT0TNAS0cfR1q9w3RH2nrp6Rs88dXU2jNmIcrOCpGX\n7U1SyM3OICcrRE5WiILcTGbPzGfOzDxml+VTlJ+liQrjSHRxqQRG3savHljlo00lMMdHX4BbgAcm\nHamITHvZWSGys3IpK8llwezx2w4MDtHZM8jR9l4OHvFWPzh0tJvegSHaOvo40tZDT9/QmP2DgQCF\neZkU5Xsz4vJzMynIyfD+jS7FU5iXRXlpbsLWhJvKEl1c/F6heVpZN7O/B/qdcz+YqG04XHg6L5GS\nlIsY5SJGufhDw8MRevu90U5LRx/1TZ3UH+6gvqmTY+29tEaL0IFxDscB5GSFmD+7iMpwAeFS7xzQ\nzGLv4tSCaBEqyM1MqXNCiS4uDcDICfZVeCOQ8drMjbbJHK+vmd0ErAOu8hOIrrj16OrjGOUiRrmI\nGSsXxdkhiquKWVZV/AfPDQwOn5iW3dkzcOKrraufQ0e7qG/qpPZAK25fyx/0PS4jFKS0MIvS6DVC\n+TkZ5B6/hig6MWFWae4ZvbPpZP7gSHSUm4EaM6sGDgLXAzeMarMRuA140MxWA63OucNmdnSsvtFZ\nZH8DXO6c603wexARGVdmRpDigmyKC7LHbDM4NMzR9thsudbOPjp7BujqGYwWoj6OdfTx5oHWcQ/5\nZGYEycvJ8FbCzsmgMC+LojxvgkJOVgbZmd45osyMIBkh7yszFCAYDBAKBcnKCFJWnEteTmI//hO6\nd+fcoJndBjyON534O865nWa2Ifr83c65x8xsnZnVAl3AzeP1je76G0AW8ISZAbzgnLs1ke9FRGQy\nMkL+ZssNDg3T1eONgrp7B+no7qc5OjGhubWXzh5vodL26KjodJeHLM7PomJGHoV5mWSEvGuKqsIF\nvHPlvNPb4ShauDLN6PBHjHIRo1zETKdcDEcidPUM0NE9QEd3P30DQ/T2e18Dg8MMDh3/ipy4hqi/\nf5jDrd607qNtvW8ZJeXnZHDHpy47sXK2Fq4UEUlD3ow1b/kcOPVrdAYGh+jpG2JoOMLg0DAFuZlx\nuyWDiouISJrKzAglbOkc3TVIRETiTsVFRETiTsVFRETiTsVFRETiTsVFRETiTsVFRETiTsVFRETi\nTsVFRETiTsVFRETiTsVFRETiTsVFRETiTsVFRETiTsVFRETiTsVFRETiTsVFRETiTsVFRETiTsVF\nRETiTsVFRETiTsVFRETiTsVFRETiLiOROzezNcAdQAi4xzl3+0na3AmsBbqBm5xzW8bra2YzgIeA\n+UAd8AHnXGsi34eIiJyahI1czCwE3AWsAZYCN5jZklFt1gELnXM1wMeAb/no+zngCefcIuDJ6LaI\niEwhiTwsthKodc7VOecGgAeB9aPaXAfcD+Cc2wSUmFnFBH1P9In++94EvgcRETkNiSwulcCBEdv1\n0cf8tJkzTt9y59zh6PeHgfJ4BSwiIvGRyOIS8dku4LPNH+zPORc5hdcREZEzJJEn9BuAqhHbVXgj\nkPHazI22yTzJ4w3R7w+bWYVzrtHMZgNNPmIJhMOFpxJ7SlMuYpSLGOUiRrmYvESOXDYDNWZWbWZZ\nwPXAxlFtNgIfATCz1UBr9JDXeH03AjdGv78R+GkC34OIiJyGhBUX59wgcBvwOLADeMg5t9PMNpjZ\nhmibx4A9ZlYL3A3cOl7f6K6/ClxjZruAK6PbIiIyhQQiEZ2yEBGR+NIV+iIiEncqLiIiEncqLiIi\nEncJXVss2fysbZaqzKwK+C4wC+9aoP9wzt2ZzmuzRZcV2gzUO+fek665MLMS4B5gGd7Pxs3Am6Rn\nLv4K+CheHrbj5SKfNMiFmd0LXAs0OefOiT425u+EmX0euAUYAj7lnPvVePtP2ZGLn7XNUtwA8FfO\nuWXAauCT0fefzmuz/SXe7MPjs1jSNRdfBx5zzi0BzgXeIA1zYWaVwF8AF0Y/XEPAB0mfXNyH9/k4\n0knfu5ktxbskZGm0zzfNbNz6kbLFBX9rm6Us51yjc+7V6PedwE68JXTScm02M5sLrMP7i/34qhBp\nlwszKwbe7py7F7xp/865NtIwF1EZQJ6ZZQB5wEHSJBfOuWeAllEPj/Xe1wMPOOcGnHN1QC3eZ+yY\nUrm4+FnbLC2YWTWwAthE+q7N9m/A3wDDIx5Lx1wsAJrN7D4ze8XMvm1m+aRhLpxzDcC/APvxikqr\nc+4J0jAXI4z13ufw1hVWJvw8TeXiogt4ADMrAB4G/tI51zHyuXRZm83M3o13XHkLY6xlly65wPtL\n/QLgm865C4AuRh32SZdcmFkp3l/q1XgfngVm9l9HtkmXXJyMj/c+bl5Subj4WdsspZlZJl5h+Z5z\n7vgyOYejtzXgFNZmm+7eBlxnZnuBB4Arzex7pGcu6vEmNLwU3f4RXrFpTMNcXA3sdc4dja4K8mPg\nEtIzF8eN9TtxsnUgGxhHKhcXP2ubpSwzCwDfAXY45+4Y8VTarc3mnPs751yVc24B3gnb3zjn/pT0\nzEUjcMDMFkUfuhp4HXiENMsFsA9YbWa50d+Xq/EmfKRjLo4b63diI/BBM8syswVADfDieDtK6eVf\nzGwtsanI33HOfSXJIZ0xZnYZ8DSwjdjw9fN4PxD/CcwjhadZjsXMLgf+2jl3XXTaZdrlwszOw5vY\nkAXsxpt+GyI9c/ElvD88B4FXgD8DCkmDXJjZA8DlQBne+ZV/BH7GGO/dzP4ObyryIN5h9sfH239K\nFxcREUmOVD4sJiIiSaLiIiIicafiIiIicafiIiIicafiIiIicafiIiIicafiIiIicafiIiIicff/\nAbXwG4e7d3GbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x908b750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.plot(range(20),lsa.explained_variance_ratio )\n",
    "plt.plot(range(100),lsa.explained_variance_ratio_)\n",
    "plt.ylabel('sqrt eigenvalue')\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each doc is sentence level\n",
    "- should have used paragraph level docs, to get to a more relevant state\n",
    "- But for now 10 would do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code from: http://stackoverflow.com/questions/31847682/how-to-compute-skipgrams-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30145"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txtList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "import copy\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def pad_sequence(sequence, n, pad_left=False, pad_right=False, pad_symbol=None):\n",
    "    if pad_left:\n",
    "        sequence = chain((pad_symbol,) * (n-1), sequence)\n",
    "    if pad_right:\n",
    "        sequence = chain(sequence, (pad_symbol,) * (n-1))\n",
    "    return sequence\n",
    "\n",
    "def skipgrams(sequence, n, k, pad_left=False, pad_right=False, pad_symbol=None):\n",
    "    sequence_length = len(sequence)\n",
    "    sequence = iter(sequence)\n",
    "    sequence = pad_sequence(sequence, n, pad_left, pad_right, pad_symbol)\n",
    "\n",
    "    if sequence_length + pad_left + pad_right < k:\n",
    "        raise Exception(\"The length of sentence + padding(s) < skip\")\n",
    "\n",
    "    if n < k:\n",
    "        raise Exception(\"Degree of Ngrams (n) needs to be bigger than skip (k)\")    \n",
    "\n",
    "    history = []\n",
    "    nk = n+k\n",
    "\n",
    "    # Return point for recursion.\n",
    "    if nk < 1: \n",
    "        return\n",
    "    # If n+k longer than sequence, reduce k by 1 and recur\n",
    "    elif nk > sequence_length: \n",
    "        for ng in skipgrams(list(sequence), n, k-1):\n",
    "            yield ng\n",
    "\n",
    "    while nk > 1: # Collects the first instance of n+k length history\n",
    "        history.append(next(sequence))\n",
    "        nk -= 1\n",
    "\n",
    "    # Iterative drop first item in history and picks up the next\n",
    "    # while yielding skipgrams for each iteration.\n",
    "    for item in sequence:\n",
    "        history.append(item)\n",
    "        current_token = history.pop(0)      \n",
    "        # Iterates through the rest of the history and \n",
    "        # pick out all combinations the n-1grams\n",
    "        for idx in list(combinations(range(len(history)), n-1)):\n",
    "            ng = [current_token]\n",
    "            for _id in idx:\n",
    "                ng.append(history[_id])\n",
    "            yield tuple(ng)\n",
    "\n",
    "    # Recursively yield the skigrams for the rest of seqeunce where\n",
    "    # len(sequence) < n+k\n",
    "    for ng in list(skipgrams(history, n, k-1)):\n",
    "        yield ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Insurgents', 'killed', 'in'),\n",
       " ('Insurgents', 'killed', 'ongoing'),\n",
       " ('Insurgents', 'killed', 'fighting'),\n",
       " ('Insurgents', 'in', 'ongoing'),\n",
       " ('Insurgents', 'in', 'fighting'),\n",
       " ('Insurgents', 'ongoing', 'fighting'),\n",
       " ('killed', 'in', 'ongoing'),\n",
       " ('killed', 'in', 'fighting'),\n",
       " ('killed', 'ongoing', 'fighting'),\n",
       " ('in', 'ongoing', 'fighting')]"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_skip_bigrams = list(skipgrams(['Insurgents','killed','in','ongoing','fighting'], n=3, k=3))\n",
    "two_skip_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- clean up all sentences in txtList\n",
    "- remember that txtList is a list of sentences where we have from the 2008 campaign speeches\n",
    "- I want to get all the individual documents\n",
    "- Once user puts in a string or set of strings, I want to see which topic they are likely to belong to\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wherever I go I talk about how we need to bring about real change in this country',\n",
       " 'And few understand the need for change as well as folks here in Michigan',\n",
       " 'Because while weve been talking about a recession in this country for a few months now Michigan has been living it for a very long time',\n",
       " 'Michigan has the highest unemployment rate in the nation and workers and communities across this state have been struggling for years with the downturn that all of America is feeling today',\n",
       " 'In fairness some of these challenges are the product of larger forces beyond the control of government']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedTxtList = []\n",
    "\n",
    "def cleanUpText(textList):\n",
    "    for sentence in textList:\n",
    "        for punct in string.punctuation:\n",
    "            sentence = sentence.replace(punct, '')\n",
    "        sentence = sentence.strip()\n",
    "        cleanedTxtList.append(sentence)\n",
    "    return cleanedTxtList\n",
    "\n",
    "cleanUpText(txtList) \n",
    "cleanedTxtList[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer( min_df=1, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = vectorizer.fit_transform(topicDict[9]).toarray()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2552"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(x, columns=vectorizer.get_feature_names())\n",
    "df[df['iraq'] > 0]['iraq'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about a function that gives the count of word in all topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def wordInTopicCounter(word, topicDict):\n",
    "    for num in range(10):\n",
    "        wordFreqInTopicList = []\n",
    "        x = vectorizer.fit_transform(topicDict[num]).toarray()\n",
    "        df = pd.DataFrame(x, columns=vectorizer.get_feature_names())\n",
    "        \n",
    "        if word in vectorizer.get_feature_names():\n",
    "            wordFreqInTopicList.append(df[df[word] > 0][word].count())\n",
    "        else:\n",
    "            wordFreqInTopicList.append(0)\n",
    "            \n",
    "    return wordFreqInTopicList\n",
    "\n",
    "wordInTopicCounter('iraq', topicDict)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = vectorizer.fit_transform(topicDict[3]).toarray()\n",
    "df = pd.DataFrame(x, columns=vectorizer.get_feature_names())\n",
    "df[df['iraq'] > 0]['iraq'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------------------------------- Rough -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I have stared the day tired',\n",
       " ' But I will not let it get to me',\n",
       " ' This is the beginning of beginning',\n",
       " '']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docStr = 'I have stared the day tired. But I will not let it get to me. This is the beginning of beginning.'\n",
    "docList = docStr.split('.')\n",
    "docList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hence if we split a string by a punctuation, that punctuation does not show up as element in generated list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
