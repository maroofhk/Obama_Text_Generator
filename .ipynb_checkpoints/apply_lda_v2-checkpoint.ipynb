{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generic\n",
    "import os\n",
    "import string\n",
    "from operator import itemgetter\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# text\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "# sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2008_44_pid=76232.txt'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getcwd()+'\\\\data'\n",
    "fileList = os.listdir(path)\n",
    "\n",
    "fileList[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank you, Iowa',\n",
       " ' You know, they said this day would never come',\n",
       " ' They said our sights were set too high',\n",
       " ' They said this country was too divided; too disillusioned to ever come together around a common purpose',\n",
       " \" But on this January night - at this defining moment in history - you have done what the cynics said we couldn't do\",\n",
       " ' You have done what the state of New Hampshire can do in five days',\n",
       " ' You have done what America can do in this New Year, 2008',\n",
       " ' In lines that stretched around schools and churches; in small towns and big cities; you came together as Democrats, Republicans and Independents to stand up and say that we are one nation; we are one people; and our time for change has come',\n",
       " \" You said the time has come to move beyond the bitterness and pettiness and anger that's consumed Washington; to end the political strategy that's been all about division and instead make it about addition - to build a coalition for change that stretches through Red States and Blue States\",\n",
       " \" Because that's how we'll win in November, and that's how we'll finally meet the challenges that we face as a nation\",\n",
       " ' We are choosing hope over fear',\n",
       " \" We're choosing unity over division, and sending a powerful message that change is coming to America\",\n",
       " \" You said the time has come to tell the lobbyists who think their money and their influence speak louder than our voices that they don't own this government, we do; and we are here to take it back\",\n",
       " \" The time has come for a President who will be honest about the choices and the challenges we face; who will listen to you and learn from you even when we disagree; who won't just tell you what you want to hear, but what you need to know\",\n",
       " ' And in New Hampshire, if you give me the same chance that Iowa did tonight, I will be that president for America',\n",
       " ' Thank you',\n",
       " \" I'll be a President who finally makes health care affordable and available to every single American the same way I expanded health care in Illinois - by--by bringing Democrats and Republicans together to get the job done\",\n",
       " \" I'll be a President who ends the tax breaks for companies that ship our jobs overseas and put a middle-class tax cut into the pockets of the working Americans who deserve it\",\n",
       " \" I'll be a President who harnesses the ingenuity of farmers and scientists and entrepreneurs to free this nation from the tyranny of oil once and for all\",\n",
       " \" And I'll be a President who ends this war in Iraq and finally brings our troops home; who restores our moral standing; who understands that 9/11 is not a way to scare up votes, but a challenge that should unite America and the world against the common threats of the twenty-first century; common threats of terrorism and nuclear weapons; climate change and poverty; genocide and disease\",\n",
       " ' Tonight, we are one step closer to that vision of America because of what you did here in Iowa',\n",
       " \" And so I'd especially like to thank the organizers and the precinct captains; the volunteers and the staff who made this all possible\",\n",
       " ' And while I\\'m at it, on \"thank yous,\" I think it makes sense for me to thank the love of my life, the rock of the Obama family, the closer on the campaign trail; give it up for Michelle Obama',\n",
       " \" I know you didn't do this for me\",\n",
       " ' You did this-you did this because you believed so deeply in the most American of ideas - that in the face of impossible odds, people who love this country can change it',\n",
       " \" I know this-I know this because while I may be standing here tonight, I'll never forget that my journey began on the streets of Chicago doing what so many of you have done for this campaign and all the campaigns here in Iowa - organizing, and working, and fighting to make people's lives just a little bit better\",\n",
       " ' I know how hard it is',\n",
       " ' It comes with little sleep, little pay, and a lot of sacrifice',\n",
       " \" There are days of disappointment, but sometimes, just sometimes, there are nights like this - a night-a night that, years from now, when we've made the changes we believe in; when more families can afford to see a doctor; when our children-when Malia and Sasha and your children-inherit a planet that's a little cleaner and safer; when the world sees America differently, and America sees itself as a nation less divided and more united; you'll be able to look back with pride and say that this was the moment when it all began\",\n",
       " ' This was the moment when the improbable beat what Washington always said was inevitable',\n",
       " \" This was the moment when we tore down barriers that have divided us for too long - when we rallied people of all parties and ages to a common cause; when we finally gave Americans who'd never participated in politics a reason to stand up and to do so\",\n",
       " ' This was the moment when we finally beat back the politics of fear, and doubt, and cynicism; the politics where we tear each other down instead of lifting this country up',\n",
       " ' This was the moment',\n",
       " \" Years from now, you'll look back and you'll say that this was the moment - this was the place - where America remembered what it means to hope\",\n",
       " \" For many months, we've been teased, even derided for talking about hope\",\n",
       " ' But we always knew that hope is not blind optimism',\n",
       " \" It's not ignoring the enormity of the task ahead or the roadblocks that stand in our path\",\n",
       " \" It's not sitting on the sidelines or shirking from a fight\",\n",
       " ' Hope is that thing inside us that insists, despite all evidence to the contrary, that something better awaits us if we have the courage to reach for it, and to work for it, and to fight for it',\n",
       " \" Hope is what I saw in the eyes of the young woman in Cedar Rapids who works the night shift after a full day of college and still can't afford health care for a sister who's ill; a young woman who still believes that this country will give her the chance to live out her dreams\",\n",
       " \" Hope is what I heard in the voice of the New Hampshire woman who told me that she hasn't been able to breathe since her nephew left for Iraq; who still goes to bed each night praying for his safe return\",\n",
       " \" Hope is what led a band of colonists to rise up against an empire; what led the greatest of generations to free a continent and heal a nation; what led young women and young men to sit at lunch counters and brave fire hoses and march through Selma and Montgomery for freedom's cause\",\n",
       " ' Hope-hope-is what led me here today - with a father from Kenya; a mother from Kansas; and a story that could only happen in the United States of America',\n",
       " ' Hope is the bedrock of this nation; the belief that our destiny will not be written for us, but by us; by all those men and women who are not content to settle for the world as it is; who have the courage to remake the world as it should be',\n",
       " ' That is what we started here in Iowa, and that is the message we can now carry to New Hampshire and beyond; the same message we had when we were up and when we were down; the one that can change this country brick by brick, block by block, calloused hand by calloused hand - that together, ordinary people can do extraordinary things; because we are not a collection of Red States and Blue States, we are the United States of America; and at this moment, in this election, we are ready to believe again',\n",
       " ' Thank you, Iowa',\n",
       " '']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathToFile = path + '\\\\' + fileList[3]\n",
    "with open(pathToFile, 'r') as f:\n",
    "    txt = f.readlines()\n",
    "    \n",
    "txtList = txt[0].split('.')\n",
    "txtList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank Iowa',\n",
       " 'You know said day would never come',\n",
       " 'They said sights set high',\n",
       " 'They said country divided disillusioned ever come together around common purpose',\n",
       " 'But January night defining moment history done cynics said couldnt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "stop += ['.', ',', '(', ')', \"'\", '\"']\n",
    "\n",
    "def stringClean(str):\n",
    "    str = ''.join([i for i in str if i not in string.punctuation])\n",
    "    words = TextBlob(str).words\n",
    "    words = ' '.join([w for w in words if w not in stop if len(w)> 1])\n",
    "    \n",
    "    return words\n",
    "\n",
    "updatedTextList = []\n",
    "for txt in txtList:\n",
    "    updatedTextList.append(stringClean(txt))\n",
    "    \n",
    "updatedTextList[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='\\\\b[a-z][a-z]+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a CountVectorizer for parsing/counting words\n",
    "# make sure we are breaking out by word and not character.\n",
    "doc_term_vector = CountVectorizer(analyzer='word',\n",
    "                                  ngram_range=(1,2),\n",
    "                                  stop_words='english',\n",
    "                                  token_pattern = '\\\\b[a-z][a-z]+\\\\b'\n",
    "                                 )\n",
    "\n",
    "doc_term_vector.fit(txtList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able', 'able breathe', 'able look', 'addition', 'addition build']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_vector.get_feature_names()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(823, 47)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the term-document matrix\n",
    "# Transpose it so the terms are the rows\n",
    "term_docs = doc_term_vector.transform(updatedTextList).transpose()\n",
    "term_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "# here corpus will be fed into lda in iteration\n",
    "corpus = matutils.Sparse2Corpus(term_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2word = dict((v,k) for k, v in doc_term_vector.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "# we can also run lsa on this:\n",
    "# lsa = models.LsiModel(corpus=corpus, num_topics)\n",
    "lda = models.LdaModel(corpus=corpus, num_topics=10, id2word=id2word, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.013*nation + 0.009*world + 0.009*hope + 0.009*come + 0.009*time'),\n",
       " (1, '0.019*know + 0.019*america + 0.013*hope + 0.013*sees + 0.007*said'),\n",
       " (2, '0.015*led + 0.015*moment + 0.010*hope + 0.010*cause + 0.010*young'),\n",
       " (3, '0.014*states + 0.010*said + 0.010*washington + 0.010*tax + 0.005*led'),\n",
       " (4,\n",
       "  '0.017*country + 0.010*health + 0.010*care + 0.010*health care + 0.010*finally'),\n",
       " (5,\n",
       "  '0.023*iowa + 0.015*know + 0.015*thank iowa + 0.015*thank + 0.008*roadblocks'),\n",
       " (6,\n",
       "  '0.018*america + 0.012*president + 0.012*common + 0.012*common threats + 0.012*threats'),\n",
       " (7,\n",
       "  '0.015*little + 0.008*new + 0.008*sacrifice + 0.008*comes little + 0.008*pay lot'),\n",
       " (8, '0.027*thank + 0.018*obama + 0.010*closer + 0.010*love + 0.010*makes'),\n",
       " (9, '0.001*led + 0.001*young + 0.001*cause + 0.001*moment + 0.001*divided')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(num_words=5, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(408, 0.014951085364410407),\n",
       " (483, 0.014942603465743614),\n",
       " (337, 0.010129232864630223),\n",
       " (102, 0.010128715329547759),\n",
       " (817, 0.010128259597981019),\n",
       " (618, 0.0053070292962610725),\n",
       " (332, 0.0053065738275374641),\n",
       " (193, 0.0053065689793565726),\n",
       " (201, 0.0053065443770853333),\n",
       " (192, 0.0053065433258200458)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we are looking into top words that pertain to topic 0\n",
    "lda.get_topic_terms(2,topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_corpus = lda[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_docs = [doc for doc in lda_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.025000000117549978),\n",
       " (1, 0.025000721463372268),\n",
       " (2, 0.025000000083206794),\n",
       " (3, 0.025003397250346206),\n",
       " (4, 0.77499400909080118),\n",
       " (5, 0.025000000126935967),\n",
       " (6, 0.025000000125384444),\n",
       " (7, 0.025001202016736005),\n",
       " (8, 0.025000669559327497),\n",
       " (9, 0.025000000166339856)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.matutils.Sparse2Corpus at 0x7973770>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'able',\n",
       " 1: 'able breathe',\n",
       " 2: 'able look',\n",
       " 3: 'addition',\n",
       " 4: 'addition build',\n",
       " 5: 'afford',\n",
       " 6: 'afford doctor',\n",
       " 7: 'afford health',\n",
       " 8: 'affordable',\n",
       " 9: 'affordable available',\n",
       " 10: 'ages',\n",
       " 11: 'ages common',\n",
       " 12: 'ahead',\n",
       " 13: 'ahead roadblocks',\n",
       " 14: 'america',\n",
       " 15: 'america did',\n",
       " 16: 'america differently',\n",
       " 17: 'america moment',\n",
       " 18: 'america new',\n",
       " 19: 'america remembered',\n",
       " 20: 'america sees',\n",
       " 21: 'america world',\n",
       " 22: 'american',\n",
       " 23: 'american ideas',\n",
       " 24: 'american way',\n",
       " 25: 'americans',\n",
       " 26: 'americans deserve',\n",
       " 27: 'americans participated',\n",
       " 28: 'anger',\n",
       " 29: 'anger consumed',\n",
       " 30: 'available',\n",
       " 31: 'available single',\n",
       " 32: 'awaits',\n",
       " 33: 'awaits courage',\n",
       " 34: 'band',\n",
       " 35: 'band colonists',\n",
       " 36: 'barriers',\n",
       " 37: 'barriers divided',\n",
       " 38: 'beat',\n",
       " 39: 'beat politics',\n",
       " 40: 'beat washington',\n",
       " 41: 'bed',\n",
       " 42: 'bed night',\n",
       " 43: 'bedrock',\n",
       " 44: 'bedrock nation',\n",
       " 45: 'began',\n",
       " 46: 'began streets',\n",
       " 47: 'belief',\n",
       " 48: 'belief destiny',\n",
       " 49: 'believe',\n",
       " 50: 'believe families',\n",
       " 51: 'believed',\n",
       " 52: 'believed deeply',\n",
       " 53: 'believes',\n",
       " 54: 'believes country',\n",
       " 55: 'better',\n",
       " 56: 'better awaits',\n",
       " 57: 'big',\n",
       " 58: 'big cities',\n",
       " 59: 'bit',\n",
       " 60: 'bit better',\n",
       " 61: 'bitterness',\n",
       " 62: 'bitterness pettiness',\n",
       " 63: 'blind',\n",
       " 64: 'blind optimism',\n",
       " 65: 'block',\n",
       " 66: 'block block',\n",
       " 67: 'block calloused',\n",
       " 68: 'blue',\n",
       " 69: 'blue states',\n",
       " 70: 'brave',\n",
       " 71: 'brave hoses',\n",
       " 72: 'breaks',\n",
       " 73: 'breaks companies',\n",
       " 74: 'breathe',\n",
       " 75: 'breathe nephew',\n",
       " 76: 'brick',\n",
       " 77: 'brick block',\n",
       " 78: 'brick brick',\n",
       " 79: 'bringing',\n",
       " 80: 'bringing democrats',\n",
       " 81: 'brings',\n",
       " 82: 'brings troops',\n",
       " 83: 'build',\n",
       " 84: 'build coalition',\n",
       " 85: 'calloused',\n",
       " 86: 'calloused hand',\n",
       " 87: 'came',\n",
       " 88: 'came democrats',\n",
       " 89: 'campaign',\n",
       " 90: 'campaign campaigns',\n",
       " 91: 'campaign trail',\n",
       " 92: 'campaigns',\n",
       " 93: 'campaigns iowa',\n",
       " 94: 'captains',\n",
       " 95: 'captains volunteers',\n",
       " 96: 'care',\n",
       " 97: 'care affordable',\n",
       " 98: 'care illinois',\n",
       " 99: 'care sister',\n",
       " 100: 'carry',\n",
       " 101: 'carry new',\n",
       " 102: 'cause',\n",
       " 103: 'cause finally',\n",
       " 104: 'cedar',\n",
       " 105: 'cedar rapids',\n",
       " 106: 'century',\n",
       " 107: 'century common',\n",
       " 108: 'challenge',\n",
       " 109: 'challenge unite',\n",
       " 110: 'challenges',\n",
       " 111: 'challenges face',\n",
       " 112: 'chance',\n",
       " 113: 'chance iowa',\n",
       " 114: 'chance live',\n",
       " 115: 'change',\n",
       " 116: 'change come',\n",
       " 117: 'change coming',\n",
       " 118: 'change country',\n",
       " 119: 'change poverty',\n",
       " 120: 'change stretches',\n",
       " 121: 'changes',\n",
       " 122: 'changes believe',\n",
       " 123: 'chicago',\n",
       " 124: 'chicago doing',\n",
       " 125: 'children',\n",
       " 126: 'children inherit',\n",
       " 127: 'children malia',\n",
       " 128: 'choices',\n",
       " 129: 'choices challenges',\n",
       " 130: 'choosing',\n",
       " 131: 'choosing hope',\n",
       " 132: 'choosing unity',\n",
       " 133: 'churches',\n",
       " 134: 'churches small',\n",
       " 135: 'cities',\n",
       " 136: 'cities came',\n",
       " 137: 'class',\n",
       " 138: 'class tax',\n",
       " 139: 'cleaner',\n",
       " 140: 'cleaner safer',\n",
       " 141: 'climate',\n",
       " 142: 'climate change',\n",
       " 143: 'closer',\n",
       " 144: 'closer campaign',\n",
       " 145: 'closer vision',\n",
       " 146: 'coalition',\n",
       " 147: 'coalition change',\n",
       " 148: 'collection',\n",
       " 149: 'collection red',\n",
       " 150: 'college',\n",
       " 151: 'college afford',\n",
       " 152: 'colonists',\n",
       " 153: 'colonists rise',\n",
       " 154: 'come',\n",
       " 155: 'come bitterness',\n",
       " 156: 'come common',\n",
       " 157: 'come president',\n",
       " 158: 'come tell',\n",
       " 159: 'comes',\n",
       " 160: 'comes little',\n",
       " 161: 'coming',\n",
       " 162: 'coming america',\n",
       " 163: 'common',\n",
       " 164: 'common cause',\n",
       " 165: 'common purpose',\n",
       " 166: 'common threats',\n",
       " 167: 'companies',\n",
       " 168: 'companies ship',\n",
       " 169: 'consumed',\n",
       " 170: 'consumed washington',\n",
       " 171: 'content',\n",
       " 172: 'content settle',\n",
       " 173: 'continent',\n",
       " 174: 'continent heal',\n",
       " 175: 'contrary',\n",
       " 176: 'contrary better',\n",
       " 177: 'couldn',\n",
       " 178: 'counters',\n",
       " 179: 'counters brave',\n",
       " 180: 'country',\n",
       " 181: 'country brick',\n",
       " 182: 'country chance',\n",
       " 183: 'country change',\n",
       " 184: 'country divided',\n",
       " 185: 'courage',\n",
       " 186: 'courage reach',\n",
       " 187: 'courage remake',\n",
       " 188: 'cut',\n",
       " 189: 'cut pockets',\n",
       " 190: 'cynicism',\n",
       " 191: 'cynicism politics',\n",
       " 192: 'cynics',\n",
       " 193: 'cynics said',\n",
       " 194: 'day',\n",
       " 195: 'day college',\n",
       " 196: 'day come',\n",
       " 197: 'days',\n",
       " 198: 'days disappointment',\n",
       " 199: 'deeply',\n",
       " 200: 'deeply american',\n",
       " 201: 'defining',\n",
       " 202: 'defining moment',\n",
       " 203: 'democrats',\n",
       " 204: 'democrats republicans',\n",
       " 205: 'derided',\n",
       " 206: 'derided talking',\n",
       " 207: 'deserve',\n",
       " 208: 'despite',\n",
       " 209: 'despite evidence',\n",
       " 210: 'destiny',\n",
       " 211: 'destiny written',\n",
       " 212: 'did',\n",
       " 213: 'did believed',\n",
       " 214: 'did did',\n",
       " 215: 'did iowa',\n",
       " 216: 'did tonight',\n",
       " 217: 'didn',\n",
       " 218: 'differently',\n",
       " 219: 'differently america',\n",
       " 220: 'disagree',\n",
       " 221: 'disagree won',\n",
       " 222: 'disappointment',\n",
       " 223: 'disappointment just',\n",
       " 224: 'disease',\n",
       " 225: 'disillusioned',\n",
       " 226: 'disillusioned come',\n",
       " 227: 'divided',\n",
       " 228: 'divided disillusioned',\n",
       " 229: 'divided long',\n",
       " 230: 'divided united',\n",
       " 231: 'division',\n",
       " 232: 'division instead',\n",
       " 233: 'division sending',\n",
       " 234: 'doctor',\n",
       " 235: 'doctor children',\n",
       " 236: 'doing',\n",
       " 237: 'doing campaign',\n",
       " 238: 'don',\n",
       " 239: 'don government',\n",
       " 240: 'doubt',\n",
       " 241: 'doubt cynicism',\n",
       " 242: 'dreams',\n",
       " 243: 'election',\n",
       " 244: 'election ready',\n",
       " 245: 'empire',\n",
       " 246: 'empire led',\n",
       " 247: 'end',\n",
       " 248: 'end political',\n",
       " 249: 'ends',\n",
       " 250: 'ends tax',\n",
       " 251: 'ends war',\n",
       " 252: 'enormity',\n",
       " 253: 'enormity task',\n",
       " 254: 'entrepreneurs',\n",
       " 255: 'entrepreneurs free',\n",
       " 256: 'especially',\n",
       " 257: 'especially like',\n",
       " 258: 'evidence',\n",
       " 259: 'evidence contrary',\n",
       " 260: 'expanded',\n",
       " 261: 'expanded health',\n",
       " 262: 'extraordinary',\n",
       " 263: 'extraordinary things',\n",
       " 264: 'eyes',\n",
       " 265: 'eyes young',\n",
       " 266: 'face',\n",
       " 267: 'face impossible',\n",
       " 268: 'face listen',\n",
       " 269: 'face nation',\n",
       " 270: 'families',\n",
       " 271: 'families afford',\n",
       " 272: 'family',\n",
       " 273: 'family closer',\n",
       " 274: 'farmers',\n",
       " 275: 'farmers scientists',\n",
       " 276: 'father',\n",
       " 277: 'father kenya',\n",
       " 278: 'fear',\n",
       " 279: 'fear doubt',\n",
       " 280: 'fight',\n",
       " 281: 'fighting',\n",
       " 282: 'fighting make',\n",
       " 283: 'finally',\n",
       " 284: 'finally beat',\n",
       " 285: 'finally brings',\n",
       " 286: 'finally gave',\n",
       " 287: 'finally makes',\n",
       " 288: 'finally meet',\n",
       " 289: 'forget',\n",
       " 290: 'forget journey',\n",
       " 291: 'free',\n",
       " 292: 'free continent',\n",
       " 293: 'free nation',\n",
       " 294: 'freedom',\n",
       " 295: 'freedom cause',\n",
       " 296: 'gave',\n",
       " 297: 'gave americans',\n",
       " 298: 'generations',\n",
       " 299: 'generations free',\n",
       " 300: 'genocide',\n",
       " 301: 'genocide disease',\n",
       " 302: 'goes',\n",
       " 303: 'goes bed',\n",
       " 304: 'government',\n",
       " 305: 'greatest',\n",
       " 306: 'greatest generations',\n",
       " 307: 'hampshire',\n",
       " 308: 'hampshire chance',\n",
       " 309: 'hampshire days',\n",
       " 310: 'hampshire message',\n",
       " 311: 'hampshire woman',\n",
       " 312: 'hand',\n",
       " 313: 'hand calloused',\n",
       " 314: 'hand ordinary',\n",
       " 315: 'happen',\n",
       " 316: 'happen united',\n",
       " 317: 'hard',\n",
       " 318: 'harnesses',\n",
       " 319: 'harnesses ingenuity',\n",
       " 320: 'hasn',\n",
       " 321: 'hasn able',\n",
       " 322: 'heal',\n",
       " 323: 'heal nation',\n",
       " 324: 'health',\n",
       " 325: 'health care',\n",
       " 326: 'hear',\n",
       " 327: 'hear need',\n",
       " 328: 'heard',\n",
       " 329: 'heard voice',\n",
       " 330: 'high',\n",
       " 331: 'history',\n",
       " 332: 'history cynics',\n",
       " 333: 'home',\n",
       " 334: 'home restores',\n",
       " 335: 'honest',\n",
       " 336: 'honest choices',\n",
       " 337: 'hope',\n",
       " 338: 'hope bedrock',\n",
       " 339: 'hope blind',\n",
       " 340: 'hope fear',\n",
       " 341: 'hope heard',\n",
       " 342: 'hope hope',\n",
       " 343: 'hope led',\n",
       " 344: 'hope saw',\n",
       " 345: 'hope thing',\n",
       " 346: 'hoses',\n",
       " 347: 'hoses march',\n",
       " 348: 'ideas',\n",
       " 349: 'ideas face',\n",
       " 350: 'ignoring',\n",
       " 351: 'ignoring enormity',\n",
       " 352: 'ill',\n",
       " 353: 'ill young',\n",
       " 354: 'illinois',\n",
       " 355: 'illinois bringing',\n",
       " 356: 'impossible',\n",
       " 357: 'impossible odds',\n",
       " 358: 'improbable',\n",
       " 359: 'improbable beat',\n",
       " 360: 'independents',\n",
       " 361: 'independents stand',\n",
       " 362: 'inevitable',\n",
       " 363: 'influence',\n",
       " 364: 'influence speak',\n",
       " 365: 'ingenuity',\n",
       " 366: 'ingenuity farmers',\n",
       " 367: 'inherit',\n",
       " 368: 'inherit planet',\n",
       " 369: 'inside',\n",
       " 370: 'inside insists',\n",
       " 371: 'insists',\n",
       " 372: 'insists despite',\n",
       " 373: 'instead',\n",
       " 374: 'instead lifting',\n",
       " 375: 'instead make',\n",
       " 376: 'iowa',\n",
       " 377: 'iowa did',\n",
       " 378: 'iowa message',\n",
       " 379: 'iowa organizing',\n",
       " 380: 'iraq',\n",
       " 381: 'iraq finally',\n",
       " 382: 'iraq goes',\n",
       " 383: 'january',\n",
       " 384: 'january night',\n",
       " 385: 'job',\n",
       " 386: 'jobs',\n",
       " 387: 'jobs overseas',\n",
       " 388: 'journey',\n",
       " 389: 'journey began',\n",
       " 390: 'just',\n",
       " 391: 'just little',\n",
       " 392: 'just nights',\n",
       " 393: 'just tell',\n",
       " 394: 'kansas',\n",
       " 395: 'kansas story',\n",
       " 396: 'kenya',\n",
       " 397: 'kenya mother',\n",
       " 398: 'knew',\n",
       " 399: 'knew hope',\n",
       " 400: 'know',\n",
       " 401: 'know didn',\n",
       " 402: 'know hard',\n",
       " 403: 'know know',\n",
       " 404: 'know said',\n",
       " 405: 'know standing',\n",
       " 406: 'learn',\n",
       " 407: 'learn disagree',\n",
       " 408: 'led',\n",
       " 409: 'led band',\n",
       " 410: 'led greatest',\n",
       " 411: 'led today',\n",
       " 412: 'led young',\n",
       " 413: 'left',\n",
       " 414: 'left iraq',\n",
       " 415: 'life',\n",
       " 416: 'life rock',\n",
       " 417: 'lifting',\n",
       " 418: 'lifting country',\n",
       " 419: 'like',\n",
       " 420: 'like night',\n",
       " 421: 'like thank',\n",
       " 422: 'lines',\n",
       " 423: 'lines stretched',\n",
       " 424: 'listen',\n",
       " 425: 'listen learn',\n",
       " 426: 'little',\n",
       " 427: 'little bit',\n",
       " 428: 'little cleaner',\n",
       " 429: 'little pay',\n",
       " 430: 'little sleep',\n",
       " 431: 'live',\n",
       " 432: 'live dreams',\n",
       " 433: 'lives',\n",
       " 434: 'lives just',\n",
       " 435: 'll',\n",
       " 436: 'll able',\n",
       " 437: 'll finally',\n",
       " 438: 'll forget',\n",
       " 439: 'll look',\n",
       " 440: 'll president',\n",
       " 441: 'll say',\n",
       " 442: 'll win',\n",
       " 443: 'lobbyists',\n",
       " 444: 'lobbyists think',\n",
       " 445: 'long',\n",
       " 446: 'long rallied',\n",
       " 447: 'look',\n",
       " 448: 'look ll',\n",
       " 449: 'look pride',\n",
       " 450: 'lot',\n",
       " 451: 'lot sacrifice',\n",
       " 452: 'louder',\n",
       " 453: 'louder voices',\n",
       " 454: 'love',\n",
       " 455: 'love country',\n",
       " 456: 'love life',\n",
       " 457: 'lunch',\n",
       " 458: 'lunch counters',\n",
       " 459: 'make',\n",
       " 460: 'make addition',\n",
       " 461: 'make people',\n",
       " 462: 'makes',\n",
       " 463: 'makes health',\n",
       " 464: 'makes sense',\n",
       " 465: 'malia',\n",
       " 466: 'malia sasha',\n",
       " 467: 'march',\n",
       " 468: 'march selma',\n",
       " 469: 'means',\n",
       " 470: 'means hope',\n",
       " 471: 'meet',\n",
       " 472: 'meet challenges',\n",
       " 473: 'men',\n",
       " 474: 'men sit',\n",
       " 475: 'men women',\n",
       " 476: 'message',\n",
       " 477: 'message carry',\n",
       " 478: 'message change',\n",
       " 479: 'michelle',\n",
       " 480: 'michelle obama',\n",
       " 481: 'middle',\n",
       " 482: 'middle class',\n",
       " 483: 'moment',\n",
       " 484: 'moment began',\n",
       " 485: 'moment election',\n",
       " 486: 'moment finally',\n",
       " 487: 'moment history',\n",
       " 488: 'moment improbable',\n",
       " 489: 'moment place',\n",
       " 490: 'moment tore',\n",
       " 491: 'money',\n",
       " 492: 'money influence',\n",
       " 493: 'montgomery',\n",
       " 494: 'montgomery freedom',\n",
       " 495: 'months',\n",
       " 496: 'months ve',\n",
       " 497: 'moral',\n",
       " 498: 'moral standing',\n",
       " 499: 'mother',\n",
       " 500: 'mother kansas',\n",
       " 501: 'nation',\n",
       " 502: 'nation belief',\n",
       " 503: 'nation divided',\n",
       " 504: 'nation led',\n",
       " 505: 'nation people',\n",
       " 506: 'nation tyranny',\n",
       " 507: 'need',\n",
       " 508: 'need know',\n",
       " 509: 'nephew',\n",
       " 510: 'nephew left',\n",
       " 511: 'new',\n",
       " 512: 'new hampshire',\n",
       " 513: 'new year',\n",
       " 514: 'night',\n",
       " 515: 'night defining',\n",
       " 516: 'night night',\n",
       " 517: 'night praying',\n",
       " 518: 'night shift',\n",
       " 519: 'night years',\n",
       " 520: 'nights',\n",
       " 521: 'nights like',\n",
       " 522: 'november',\n",
       " 523: 'november ll',\n",
       " 524: 'nuclear',\n",
       " 525: 'nuclear weapons',\n",
       " 526: 'obama',\n",
       " 527: 'obama family',\n",
       " 528: 'odds',\n",
       " 529: 'odds people',\n",
       " 530: 'oil',\n",
       " 531: 'optimism',\n",
       " 532: 'ordinary',\n",
       " 533: 'ordinary people',\n",
       " 534: 'organizers',\n",
       " 535: 'organizers precinct',\n",
       " 536: 'organizing',\n",
       " 537: 'organizing working',\n",
       " 538: 'overseas',\n",
       " 539: 'overseas middle',\n",
       " 540: 'participated',\n",
       " 541: 'participated politics',\n",
       " 542: 'parties',\n",
       " 543: 'parties ages',\n",
       " 544: 'path',\n",
       " 545: 'pay',\n",
       " 546: 'pay lot',\n",
       " 547: 'people',\n",
       " 548: 'people extraordinary',\n",
       " 549: 'people lives',\n",
       " 550: 'people love',\n",
       " 551: 'people parties',\n",
       " 552: 'people time',\n",
       " 553: 'pettiness',\n",
       " 554: 'pettiness anger',\n",
       " 555: 'place',\n",
       " 556: 'place america',\n",
       " 557: 'planet',\n",
       " 558: 'planet little',\n",
       " 559: 'pockets',\n",
       " 560: 'pockets working',\n",
       " 561: 'political',\n",
       " 562: 'political strategy',\n",
       " 563: 'politics',\n",
       " 564: 'politics fear',\n",
       " 565: 'politics reason',\n",
       " 566: 'politics tear',\n",
       " 567: 'possible',\n",
       " 568: 'poverty',\n",
       " 569: 'poverty genocide',\n",
       " 570: 'powerful',\n",
       " 571: 'powerful message',\n",
       " 572: 'praying',\n",
       " 573: 'praying safe',\n",
       " 574: 'precinct',\n",
       " 575: 'precinct captains',\n",
       " 576: 'president',\n",
       " 577: 'president america',\n",
       " 578: 'president ends',\n",
       " 579: 'president finally',\n",
       " 580: 'president harnesses',\n",
       " 581: 'president honest',\n",
       " 582: 'pride',\n",
       " 583: 'pride say',\n",
       " 584: 'purpose',\n",
       " 585: 'rallied',\n",
       " 586: 'rallied people',\n",
       " 587: 'rapids',\n",
       " 588: 'rapids works',\n",
       " 589: 'reach',\n",
       " 590: 'reach work',\n",
       " 591: 'ready',\n",
       " 592: 'ready believe',\n",
       " 593: 'reason',\n",
       " 594: 'reason stand',\n",
       " 595: 'red',\n",
       " 596: 'red states',\n",
       " 597: 'remake',\n",
       " 598: 'remake world',\n",
       " 599: 'remembered',\n",
       " 600: 'remembered means',\n",
       " 601: 'republicans',\n",
       " 602: 'republicans independents',\n",
       " 603: 'republicans job',\n",
       " 604: 'restores',\n",
       " 605: 'restores moral',\n",
       " 606: 'return',\n",
       " 607: 'rise',\n",
       " 608: 'rise empire',\n",
       " 609: 'roadblocks',\n",
       " 610: 'roadblocks stand',\n",
       " 611: 'rock',\n",
       " 612: 'rock obama',\n",
       " 613: 'sacrifice',\n",
       " 614: 'safe',\n",
       " 615: 'safe return',\n",
       " 616: 'safer',\n",
       " 617: 'safer world',\n",
       " 618: 'said',\n",
       " 619: 'said couldn',\n",
       " 620: 'said country',\n",
       " 621: 'said day',\n",
       " 622: 'said inevitable',\n",
       " 623: 'said sights',\n",
       " 624: 'said time',\n",
       " 625: 'sasha',\n",
       " 626: 'sasha children',\n",
       " 627: 'saw',\n",
       " 628: 'saw eyes',\n",
       " 629: 'say',\n",
       " 630: 'say moment',\n",
       " 631: 'say nation',\n",
       " 632: 'scare',\n",
       " 633: 'scare votes',\n",
       " 634: 'schools',\n",
       " 635: 'schools churches',\n",
       " 636: 'scientists',\n",
       " 637: 'scientists entrepreneurs',\n",
       " 638: 'sees',\n",
       " 639: 'sees america',\n",
       " 640: 'sees nation',\n",
       " 641: 'selma',\n",
       " 642: 'selma montgomery',\n",
       " 643: 'sending',\n",
       " 644: 'sending powerful',\n",
       " 645: 'sense',\n",
       " 646: 'sense thank',\n",
       " 647: 'set',\n",
       " 648: 'set high',\n",
       " 649: 'settle',\n",
       " 650: 'settle world',\n",
       " 651: 'shift',\n",
       " 652: 'shift day',\n",
       " 653: 'ship',\n",
       " 654: 'ship jobs',\n",
       " 655: 'shirking',\n",
       " 656: 'shirking fight',\n",
       " 657: 'sidelines',\n",
       " 658: 'sidelines shirking',\n",
       " 659: 'sights',\n",
       " 660: 'sights set',\n",
       " 661: 'single',\n",
       " 662: 'single american',\n",
       " 663: 'sister',\n",
       " 664: 'sister ill',\n",
       " 665: 'sit',\n",
       " 666: 'sit lunch',\n",
       " 667: 'sitting',\n",
       " 668: 'sitting sidelines',\n",
       " 669: 'sleep',\n",
       " 670: 'sleep little',\n",
       " 671: 'small',\n",
       " 672: 'small towns',\n",
       " 673: 'speak',\n",
       " 674: 'speak louder',\n",
       " 675: 'staff',\n",
       " 676: 'staff possible',\n",
       " 677: 'stand',\n",
       " 678: 'stand path',\n",
       " 679: 'stand say',\n",
       " 680: 'standing',\n",
       " 681: 'standing tonight',\n",
       " 682: 'standing understands',\n",
       " 683: 'started',\n",
       " 684: 'started iowa',\n",
       " 685: 'state',\n",
       " 686: 'state new',\n",
       " 687: 'states',\n",
       " 688: 'states america',\n",
       " 689: 'states blue',\n",
       " 690: 'states united',\n",
       " 691: 'step',\n",
       " 692: 'step closer',\n",
       " 693: 'story',\n",
       " 694: 'story happen',\n",
       " 695: 'strategy',\n",
       " 696: 'strategy division',\n",
       " 697: 'streets',\n",
       " 698: 'streets chicago',\n",
       " 699: 'stretched',\n",
       " 700: 'stretched schools',\n",
       " 701: 'stretches',\n",
       " 702: 'stretches red',\n",
       " 703: 'talking',\n",
       " 704: 'talking hope',\n",
       " 705: 'task',\n",
       " 706: 'task ahead',\n",
       " 707: 'tax',\n",
       " 708: 'tax breaks',\n",
       " 709: 'tax cut',\n",
       " 710: 'tear',\n",
       " 711: 'tear instead',\n",
       " 712: 'teased',\n",
       " 713: 'teased derided',\n",
       " 714: 'tell',\n",
       " 715: 'tell lobbyists',\n",
       " 716: 'tell want',\n",
       " 717: 'terrorism',\n",
       " 718: 'terrorism nuclear',\n",
       " 719: 'thank',\n",
       " 720: 'thank iowa',\n",
       " 721: 'thank love',\n",
       " 722: 'thank organizers',\n",
       " 723: 'thank yous',\n",
       " 724: 'thing',\n",
       " 725: 'thing inside',\n",
       " 726: 'things',\n",
       " 727: 'things collection',\n",
       " 728: 'think',\n",
       " 729: 'think makes',\n",
       " 730: 'think money',\n",
       " 731: 'threats',\n",
       " 732: 'threats century',\n",
       " 733: 'threats terrorism',\n",
       " 734: 'time',\n",
       " 735: 'time change',\n",
       " 736: 'time come',\n",
       " 737: 'today',\n",
       " 738: 'today father',\n",
       " 739: 'told',\n",
       " 740: 'told hasn',\n",
       " 741: 'tonight',\n",
       " 742: 'tonight ll',\n",
       " 743: 'tonight president',\n",
       " 744: 'tonight step',\n",
       " 745: 'tore',\n",
       " 746: 'tore barriers',\n",
       " 747: 'towns',\n",
       " 748: 'towns big',\n",
       " 749: 'trail',\n",
       " 750: 'trail michelle',\n",
       " 751: 'troops',\n",
       " 752: 'troops home',\n",
       " 753: 'tyranny',\n",
       " 754: 'tyranny oil',\n",
       " 755: 'understands',\n",
       " 756: 'understands way',\n",
       " 757: 'unite',\n",
       " 758: 'unite america',\n",
       " 759: 'united',\n",
       " 760: 'united ll',\n",
       " 761: 'united states',\n",
       " 762: 'unity',\n",
       " 763: 'unity division',\n",
       " 764: 've',\n",
       " 765: 've changes',\n",
       " 766: 've teased',\n",
       " 767: 'vision',\n",
       " 768: 'vision america',\n",
       " 769: 'voice',\n",
       " 770: 'voice new',\n",
       " 771: 'voices',\n",
       " 772: 'voices don',\n",
       " 773: 'volunteers',\n",
       " 774: 'volunteers staff',\n",
       " 775: 'votes',\n",
       " 776: 'votes challenge',\n",
       " 777: 'want',\n",
       " 778: 'want hear',\n",
       " 779: 'war',\n",
       " 780: 'war iraq',\n",
       " 781: 'washington',\n",
       " 782: 'washington end',\n",
       " 783: 'washington said',\n",
       " 784: 'way',\n",
       " 785: 'way expanded',\n",
       " 786: 'way scare',\n",
       " 787: 'weapons',\n",
       " 788: 'weapons climate',\n",
       " 789: 'win',\n",
       " 790: 'win november',\n",
       " 791: 'woman',\n",
       " 792: 'woman believes',\n",
       " 793: 'woman cedar',\n",
       " 794: 'woman told',\n",
       " 795: 'women',\n",
       " 796: 'women content',\n",
       " 797: 'women young',\n",
       " 798: 'won',\n",
       " 799: 'won just',\n",
       " 800: 'work',\n",
       " 801: 'work fight',\n",
       " 802: 'working',\n",
       " 803: 'working americans',\n",
       " 804: 'working fighting',\n",
       " 805: 'works',\n",
       " 806: 'works night',\n",
       " 807: 'world',\n",
       " 808: 'world common',\n",
       " 809: 'world courage',\n",
       " 810: 'world sees',\n",
       " 811: 'written',\n",
       " 812: 'written men',\n",
       " 813: 'year',\n",
       " 814: 'years',\n",
       " 815: 'years ll',\n",
       " 816: 'years ve',\n",
       " 817: 'young',\n",
       " 818: 'young men',\n",
       " 819: 'young woman',\n",
       " 820: 'young women',\n",
       " 821: 'yous',\n",
       " 822: 'yous think'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# above we see that word id 376 has the highest\n",
    "# probability of being in topic 0\n",
    "# from below the word is iowa\n",
    "lda.id2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how about we combined many texts togather and try this out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()+'\\\\data'\n",
    "fileList = os.listdir(path)\n",
    "\n",
    "# in the part of file 'speech_data_pull_v3.ipynb' I have \n",
    "pathToFile = path + '\\\\' + 'bulkSpeeches.txt'\n",
    "with open(pathToFile, 'r') as f:\n",
    "    txt = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wherever I go, I talk about how we need to bring about real change in this country',\n",
       " ' And few understand the need for change as well as folks here in Michigan',\n",
       " \" Because while we've been talking about a recession in this country for a few months now, Michigan has been living it for a very long time\",\n",
       " ' Michigan has the highest unemployment rate in the nation and workers and communities across this state have been struggling for years with the downturn that all of America is feeling today',\n",
       " ' In fairness, some of these challenges are the product of larger forces beyond the control of government']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtList = txt[0].split('.')\n",
    "txtList[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wherever go talk need bring real change country',\n",
       " 'And understand need change well folks Michigan',\n",
       " 'Because weve talking recession country months Michigan living long time',\n",
       " 'Michigan highest unemployment rate nation workers communities across state struggling years downturn America feeling today',\n",
       " 'In fairness challenges product larger forces beyond control government']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "stop += ['.', ',', '(', ')', \"'\", '\"']\n",
    "\n",
    "def stringClean(str):\n",
    "    str = ''.join([i for i in str if i not in string.punctuation])\n",
    "    words = TextBlob(str).words\n",
    "    words = ' '.join([w for w in words if w not in stop if len(w)> 1])\n",
    "    \n",
    "    return words\n",
    "\n",
    "updatedTextList = []\n",
    "for txt in txtList:\n",
    "    updatedTextList.append(stringClean(txt))\n",
    "    \n",
    "updatedTextList[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30145"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(updatedTextList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='\\\\b[a-z][a-z]+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a CountVectorizer for parsing/counting words\n",
    "# make sure we are breaking out by word and not character.\n",
    "doc_term_vector = CountVectorizer(analyzer='word',\n",
    "                                  ngram_range=(1,2),\n",
    "                                  stop_words='english',\n",
    "                                  token_pattern = '\\\\b[a-z][a-z]+\\\\b'\n",
    "                                 )\n",
    "\n",
    "doc_term_vector.fit(updatedTextList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aan', 'aan energy', 'aaron', 'aaron joshua', 'aarp']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_vector.get_feature_names()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136374, 30145)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the term-document matrix\n",
    "# Transpose it so the terms are the rows\n",
    "term_docs = doc_term_vector.transform(updatedTextList).transpose()\n",
    "term_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "# here corpus will be fed into lda in iteration\n",
    "corpus = matutils.Sparse2Corpus(term_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2word = dict((v,k) for k, v in doc_term_vector.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-e512c3d93fce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# we can also run lsa on this:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# lsa = models.LsiModel(corpus=corpus, num_topics)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mlda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Python34\\Lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLdaState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m100.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpElogbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirichlet_expectation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msstats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[1;31m# if a training corpus was provided, start estimating the model right away\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python34\\Lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36mdirichlet_expectation\u001b[1;34m(alpha)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpsi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpsi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpsi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpsi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# keep the same precision as input\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "# we can also run lsa on this:\n",
    "# lsa = models.LsiModel(corpus=corpus, num_topics)\n",
    "\n",
    "# !!! skip this part:this take a really long time, hence I am going to pickle it and run later\n",
    "\n",
    "lda = models.LdaModel(corpus=corpus, num_topics=10, id2word=id2word, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-570af72f523a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlda\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lda' is not defined"
     ]
    }
   ],
   "source": [
    "lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lda_model.pkl',\n",
       " 'lda_model.pkl_01.npy',\n",
       " 'lda_model.pkl_02.npy',\n",
       " 'lda_model.pkl_03.npy',\n",
       " 'lda_model.pkl_04.npy',\n",
       " 'lda_model.pkl_05.npy']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !!! skip this part:this take a really long time, hence I am going to pickle it and run later\n",
    "\n",
    "joblib.dump(lda, 'lda_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_model = joblib.load('lda_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*think + 0.011*going + 0.008*im + 0.008*know + 0.007*obama + 0.006*dont + 0.005*look + 0.005*make + 0.004*people + 0.003*campaign'),\n",
       " (1,\n",
       "  '0.014*care + 0.013*health + 0.012*credit + 0.012*card + 0.011*credit card + 0.010*health care + 0.008*ive + 0.007*got + 0.006*seen + 0.006*making'),\n",
       " (2,\n",
       "  '0.005*let + 0.005*bad + 0.004*rules + 0.004*america + 0.004*god + 0.004*bush + 0.004*second + 0.004*opponent + 0.004*bless + 0.004*god bless'),\n",
       " (3,\n",
       "  '0.013*tax + 0.012*mccain + 0.010*families + 0.010*john + 0.008*john mccain + 0.007*years + 0.006*americans + 0.005*senator + 0.004*thats + 0.004*dont'),\n",
       " (4,\n",
       "  '0.008*make + 0.007*sure + 0.007*make sure + 0.005*need + 0.004*ill + 0.004*security + 0.004*lets + 0.004*help + 0.003*protect + 0.003*job'),\n",
       " (5,\n",
       "  '0.006*women + 0.005*work + 0.005*men + 0.005*america + 0.005*jobs + 0.004*men women + 0.003*young + 0.003*gas + 0.003*economy + 0.003*decisions'),\n",
       " (6,\n",
       "  '0.019*thats + 0.016*change + 0.015*american + 0.013*people + 0.009*need + 0.008*president + 0.008*washington + 0.007*american people + 0.007*election + 0.007*americans'),\n",
       " (7,\n",
       "  '0.015*states + 0.012*united + 0.012*united states + 0.010*america + 0.010*obama + 0.007*president + 0.007*right + 0.005*states america + 0.005*president united + 0.004*rate'),\n",
       " (8,\n",
       "  '0.014*street + 0.013*jobs + 0.012*new + 0.010*wall + 0.010*wall street + 0.007*economy + 0.006*energy + 0.006*crisis + 0.006*create + 0.004*war'),\n",
       " (9,\n",
       "  '0.009*insurance + 0.009*senator + 0.007*taxes + 0.007*companies + 0.006*mccain + 0.005*senator mccain + 0.005*billion + 0.004*year + 0.003*know + 0.003*clinton')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(num_words=10, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# note a document = a sentence\n",
    "# we want to find which sentences belong to which topic\n",
    "\n",
    "# !!! skip this part:this take a really long time, hence I am going to pickle it and run later\n",
    "\n",
    "lda_corpus = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !!! skip this part: already pickled\n",
    "lda_corpus_pickled = joblib.dump(lda_corpus, 'lda_corpus.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda_corpus_unpickled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-3c098c37212f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#lda_corpus_unpickled = joblib.load('lda_corpus.pkl')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlda_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlda_corpus_unpickled\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lda_corpus_unpickled' is not defined"
     ]
    }
   ],
   "source": [
    "#lda_corpus_unpickled = joblib.load('lda_corpus.pkl')\n",
    "#lda_docs = [doc for doc in lda_corpus_unpickled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!!!!!!!! don't run this already pickled below\n",
    "\n",
    "lda_docs = [doc for doc in lda_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_docs_pickled = joblib.dump(lda_docs, 'model_data/lda_docs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'joblib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-bd5363a6eaa5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlda_docs_unpickled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model_data/lda_docs.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'joblib' is not defined"
     ]
    }
   ],
   "source": [
    "lda_docs_unpickled = joblib.load('model_data/lda_docs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_doc_test = lda_docs_unpickled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30145"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getSentence(sentence):\n",
    "    return sorted(sentence, key=itemgetter(1), reverse=True)[0][0]\n",
    "\n",
    "sentenceAndTopicList = []\n",
    "for index, doc in enumerate(lda_docs_unpickled):\n",
    "    temp = [index, getSentence(doc)]\n",
    "    sentenceAndTopicList.append(temp)\n",
    "    \n",
    "sentenceAndTopicList[:20]\n",
    "len(sentenceAndTopicList)\n",
    "#getSentence(lda_doc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python34\\Lib\\site-packages\\ipykernel\\__main__.py:9: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docIndex</th>\n",
       "      <th>topic</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26476</th>\n",
       "      <td>26476</td>\n",
       "      <td>1</td>\n",
       "      <td>I will reform our health care system so we ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24201</th>\n",
       "      <td>24201</td>\n",
       "      <td>1</td>\n",
       "      <td>In the Senate, I worked across the aisle to c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26477</th>\n",
       "      <td>26477</td>\n",
       "      <td>1</td>\n",
       "      <td>If you have health care, my plan will lower y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>1828</td>\n",
       "      <td>1</td>\n",
       "      <td>Other nations would feel great pressure to ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16984</th>\n",
       "      <td>16984</td>\n",
       "      <td>1</td>\n",
       "      <td>And, you know, I've seen and heard worse</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       docIndex  topic                                           sentence\n",
       "26476     26476      1   I will reform our health care system so we ca...\n",
       "24201     24201      1   In the Senate, I worked across the aisle to c...\n",
       "26477     26477      1   If you have health care, my plan will lower y...\n",
       "1828       1828      1   Other nations would feel great pressure to ac...\n",
       "16984     16984      1           And, you know, I've seen and heard worse"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def getSentence(data, textList):\n",
    "    return textList[data]\n",
    "\n",
    "df = pd.DataFrame(sentenceAndTopicList)\n",
    "df.rename(columns={0: 'docIndex', 1: 'topic'}, inplace=True)\n",
    "df['sentence'] = df.apply(lambda row: getSentence(row.docIndex, txtList), axis=1)\n",
    "df.sort(['topic'], ascending=True, inplace=True)\n",
    "df[df['topic']==1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' I will reform our health care system so we can relieve families, businesses, and our economy from the crushing cost of health care by investing in new technology and preventative care',\n",
       " ' In the Senate, I worked across the aisle to crack down on these schemes',\n",
       " ' If you have health care, my plan will lower your premiums',\n",
       " ' Other nations would feel great pressure to accommodate Iranian demands',\n",
       " \" And, you know, I've seen and heard worse\"]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic1 = []\n",
    "df_topic1 = df[df['topic'] == 1]\n",
    "topic1_sentences = list(df_topic1.iloc[:,2].values)\n",
    "\n",
    "topic1_sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This part is taken from markov.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "from collections import defaultdict\n",
    "\n",
    "# ======= (1) open a single file and upload its string into documents\n",
    "\n",
    "speech = []\n",
    "with open('obamaSpeech.txt', 'r') as f:\n",
    "    # sppech is not a list where each element is a new line\n",
    "    speech.extend(f.readlines())\n",
    "    \n",
    "document = ''.join(speech)\n",
    "\n",
    "# ======= (2) remove all punctuation except '.' which could be used to split lines later \n",
    "\n",
    "strPunctuation = string.punctuation.replace('.', '').replace(\"'\", '')\n",
    "\n",
    "# remove all punctuation\n",
    "for punct in strPunctuation:\n",
    "    document = document.replace(punct, ' ')\n",
    "\n",
    "# remove newLines, tabs and single quote\n",
    "for i in [\"\\t\",\"\\n\", \"'\"]:\n",
    "    document = document.replace(i, '')\n",
    "\n",
    "# ======= (3) Now split by '.'\n",
    "\n",
    "docList = document.split('.')\n",
    "docList = filter(lambda x: len(x)>1, docList) # get rid of any string that has length greater than 1\n",
    "docList = map(lambda x: x.strip(), docList) # strip away all empty strings at the ends\n",
    "docList = list(docList)\n",
    "\n",
    "# ======= (4) Start doing n-grams\n",
    "\n",
    "def find_ngrams(input_string, n):\n",
    "    # replace double space by single space\n",
    "    input_string = input_string.replace('  ', ' ')\n",
    "    input_list = input_string.split(' ')\n",
    "    return zip(*[input_list[i:] for i in range(n)])\n",
    "\n",
    "# ======= (5) Start doing n-grams\n",
    "\n",
    "def generateNGramDict(docList):\n",
    "    d = defaultdict(list)\n",
    "    for doc in docList:\n",
    "        trigrams = list(find_ngrams(doc,3))\n",
    "        for trigram in trigrams:\n",
    "            d[trigram[:2]].append(trigram[2])\n",
    "            \n",
    "    return d\n",
    "\n",
    "dtemp = generateNGramDict(docList)\n",
    "\n",
    "# ======= (6) Test sentence generator\n",
    "\n",
    "def generateText(triGramDict, numOfLoops, firstWord='getting', secondWord='the'):\n",
    "    newSpeech = [firstWord,secondWord]\n",
    "    counter = 0\n",
    "    \n",
    "    while counter < numOfLoops:\n",
    "        try:\n",
    "            firstWord, secondWord = secondWord, np.random.choice(triGramDict[(firstWord,secondWord)])\n",
    "            #print(newSpeech)\n",
    "            newSpeech.append(secondWord)\n",
    "        except:\n",
    "            break\n",
    "        \n",
    "    return ' '.join(newSpeech)\n",
    "\n",
    "generateText(dtemp, 10, 'I', 'want')\n",
    "#docList    \n",
    "#dtemp[('getting','the')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
